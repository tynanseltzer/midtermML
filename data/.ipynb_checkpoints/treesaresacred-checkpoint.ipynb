{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798\n",
      "[0.1913508  0.12342303 0.07756299 0.0288043  0.20031671 0.18920168\n",
      " 0.12014322 0.06919728]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "Below is our training data analysis\n",
      "635\n",
      "581\n",
      "0.9149606299212598 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "163\n",
      "147\n",
      "0.901840490797546 is our testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "import csv\n",
    "import ast\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "what = pandas.read_csv(\"partiesthush.csv\")\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "# print(df2)\n",
    "# print(df1)\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                              random_state=0)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:1324]\n",
    "\n",
    "testingx = d1.iloc[1324:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:1324]\n",
    "testingy = d2.iloc[1324:]\n",
    "\n",
    "\n",
    "# print(train)\n",
    "# trainingx = train[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# trainingy = train[['won']]\n",
    "\n",
    "# print(tester)\n",
    "\n",
    "# testingx = tester[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# testingy = tester[['won']]\n",
    "\n",
    "\n",
    "# trainingx = d1[d1.index % 4 != 0]\n",
    "# testingx = d1[d1.index % 4 == 0] \n",
    "\n",
    "# trainingy = d2[d2.index % 4 != 0]\n",
    "# testingy = d2[d2.index % 4 == 0] \n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "# # testing score\n",
    "# score = metrics.f1_score(trainingy, traininganswers, pos_label=list(set(trainingy)))\n",
    "# training score\n",
    "# score_test = metrics.f1_score(testingy, testinganswers, pos_label=list(set(testingy)))\n",
    "\n",
    "# print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Party_Previous_Vote_ShareLEFT  IncumbentLEFT  RaisedLEFT   SpentLEFT  \\\n",
      "0                         1.000000              1  1235570.11   645776.76   \n",
      "1                         0.453655              0   405460.57   267916.56   \n",
      "2                         0.329124              0   326639.05   302355.16   \n",
      "3                         1.000000              1  1452763.64   834621.99   \n",
      "4                         0.668448              1  1421378.49  1824411.31   \n",
      "5                         0.252321              0   256383.23   161621.12   \n",
      "6                         0.200000              0        0.00        0.00   \n",
      "7                         0.508008              1  2219418.25  1471073.54   \n",
      "8                         0.433324              0  3677716.16  2662198.12   \n",
      "9                         1.000000              1   593374.39   542072.37   \n",
      "10                        0.284822              0   359836.83   189839.67   \n",
      "11                        0.000000              1        0.00        0.00   \n",
      "12                        0.383773              0   297760.06   148377.42   \n",
      "13                        0.685526              1  1797520.42  1289147.72   \n",
      "14                        0.388771              0  1326295.26   938658.74   \n",
      "15                        0.765117              1   948321.16   738143.63   \n",
      "16                        0.000000              0        0.00        0.00   \n",
      "17                        0.000000              0   214334.74   198947.83   \n",
      "18                        0.000000              0   132264.52   108244.92   \n",
      "19                        0.404861              0   888363.82   456279.53   \n",
      "20                        0.770896              1   821700.49   628979.08   \n",
      "21                        0.582309              1   994729.41   708803.52   \n",
      "22                        0.627848              1  1548596.64  1061066.53   \n",
      "23                        0.752487              0        0.00        0.00   \n",
      "24                        0.488085              0   487169.13   414702.40   \n",
      "25                        0.627464              1   157625.67   149200.15   \n",
      "26                        0.423703              0    51290.50    29662.12   \n",
      "27                        0.523693              1  4087019.31  2477327.99   \n",
      "28                        0.717622              1   530980.07   424980.90   \n",
      "29                        0.810403              1  3835610.11  4236870.90   \n",
      "..                             ...            ...         ...         ...   \n",
      "370                       0.100000              0     7315.00     1808.07   \n",
      "371                       0.357731              0   353757.97   264499.02   \n",
      "372                       0.382662              0  3176278.10  1861613.40   \n",
      "373                       0.427390              0   138322.88   116554.80   \n",
      "374                       0.416833              0  2418273.78  1388629.41   \n",
      "375                       0.667180              0   725346.14   489640.75   \n",
      "376                       0.685536              1  1859623.20  1593436.52   \n",
      "377                       0.283676              0   817405.12   656241.84   \n",
      "378                       0.528552              1  5070943.06  4087964.17   \n",
      "379                       1.000000              1  1832804.72   971810.05   \n",
      "380                       0.445466              0    15000.00    11607.58   \n",
      "381                       0.640464              1  1014449.16   896923.78   \n",
      "382                       0.614009              1  2007723.86  1597999.00   \n",
      "383                       0.000000              0   410623.92   321063.77   \n",
      "384                       0.404817              0  4570326.16  3825917.34   \n",
      "385                       0.382925              0    35685.00    32812.50   \n",
      "386                       1.000000              1  1982156.60  1063131.49   \n",
      "387                       0.000000              0        0.00        0.00   \n",
      "388                       0.413322              0        0.00        0.00   \n",
      "389                       0.310264              0   224282.45   160239.69   \n",
      "390                       0.582263              1  1662294.66   623654.82   \n",
      "391                       0.678861              0  1164377.32   874822.07   \n",
      "392                       0.301986              0  7390768.76  6734529.82   \n",
      "393                       1.000000              0  2143507.25  1477010.77   \n",
      "394                       0.770405              1   895505.16   881929.06   \n",
      "395                       0.293137              0   212919.40   166937.68   \n",
      "396                       0.571977              1  1651386.31   729747.64   \n",
      "397                       0.617707              1  3349464.74  2350514.34   \n",
      "398                       0.626705              1  2631372.04  1201550.86   \n",
      "399                       0.622000              1   806920.51   624046.54   \n",
      "\n",
      "     Party_Previous_Vote_ShareRIGHT  IncumbentRIGHT  RaisedRIGHT  SpentRIGHT  \n",
      "0                          0.000000               0     76091.03    31564.26  \n",
      "1                          0.546345               1   2479632.21  2065173.18  \n",
      "2                          0.670876               1   1185228.39   610499.06  \n",
      "3                          0.000000               0     55672.52    45634.20  \n",
      "4                          0.331552               0    488944.90   365026.25  \n",
      "5                          0.747679               1   1098673.97   772846.87  \n",
      "6                          0.503000               1   1003579.47   960955.52  \n",
      "7                          0.435041               0         0.00        0.00  \n",
      "8                          0.566676               0   1201455.28  1008233.28  \n",
      "9                          0.000000               0     45488.00    30467.20  \n",
      "10                         0.000000               0         0.00        0.00  \n",
      "11                         0.368598               0    147857.31   139515.94  \n",
      "12                         0.616227               1   1493956.62  1162687.15  \n",
      "13                         0.000000               0   3705331.08  3236302.48  \n",
      "14                         0.611229               0   2125315.56  1529134.73  \n",
      "15                         0.000000               0    108134.55    82376.61  \n",
      "16                         0.368400               0   1670546.59  1440062.32  \n",
      "17                         0.773360               1   1049764.20  1019266.54  \n",
      "18                         0.748985               1   1511869.85   817718.03  \n",
      "19                         0.595139               1    809720.61   431388.01  \n",
      "20                         0.229104               0      4101.00     2894.25  \n",
      "21                         0.417691               0     25151.00    10283.01  \n",
      "22                         0.372152               0   2822139.89  1757035.12  \n",
      "23                         0.752487               1         0.00        0.00  \n",
      "24                         0.511915               0   2681013.71  1046165.92  \n",
      "25                         0.627464               0    157625.67   149200.15  \n",
      "26                         0.576297               1   1093325.75   630267.46  \n",
      "27                         0.476307               0   6045041.41  4522149.21  \n",
      "28                         0.282378               0         0.00        0.00  \n",
      "29                         0.000000               0     18983.18     5928.24  \n",
      "..                              ...             ...          ...         ...  \n",
      "370                        0.825000               1    755584.47   568834.30  \n",
      "371                        0.608917               1   1370910.04  1302877.47  \n",
      "372                        0.617338               1   3393078.65  2966376.36  \n",
      "373                        0.572610               1    781169.04   662144.72  \n",
      "374                        0.583167               0    903015.10   397925.99  \n",
      "375                        0.332820               0    331351.46   236310.06  \n",
      "376                        0.273693               0     45788.50    37520.26  \n",
      "377                        0.686937               1   1013237.60   623473.03  \n",
      "378                        0.471448               0   4542463.82  2950507.92  \n",
      "379                        0.000000               0    497329.95   448900.71  \n",
      "380                        0.554534               1   1931300.87  1193209.42  \n",
      "381                        0.000000               0         0.00        0.00  \n",
      "382                        0.385991               0   2349128.88  1274261.27  \n",
      "383                        1.000000               1   1005120.74   694974.98  \n",
      "384                        0.595183               1   5013004.00  4248373.00  \n",
      "385                        0.617075               1   2146646.68  1218478.64  \n",
      "386                        0.000000               0      5884.00     1968.00  \n",
      "387                        0.399551               0   5334751.10  3567406.63  \n",
      "388                        0.586678               1   1468361.23   948996.87  \n",
      "389                        0.689736               1   1167909.64   721418.11  \n",
      "390                        0.417737               0    599186.07   348126.05  \n",
      "391                        0.678861               0   1164377.32   874822.07  \n",
      "392                        0.650175               0   1664275.83  1032315.17  \n",
      "393                        0.000000               0    278739.68   214496.83  \n",
      "394                        0.117091               0         0.00        0.00  \n",
      "395                        0.667620               1    330350.72   334297.57  \n",
      "396                        0.372795               0   2473039.76  2080288.15  \n",
      "397                        0.382293               0     87757.31    55072.37  \n",
      "398                        0.373295               0    289999.57   180798.50  \n",
      "399                        0.300632               0     39649.68    39479.19  \n",
      "\n",
      "[400 rows x 8 columns]\n",
      "[0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
      " 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1\n",
      " 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
      " 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1\n",
      " 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
      " 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0]\n",
      "[['AL', 1, 'R'], ['AL', 2, 'R'], ['AL', 3, 'R'], ['AL', 4, 'R'], ['AL', 5, 'R'], ['AL', 6, 'R'], ['AK', 0, 'R'], ['AZ', 1, 'D'], ['AZ', 2, 'R'], ['AZ', 3, 'D'], ['AZ', 4, 'D'], ['AZ', 5, 'R'], ['AZ', 6, 'R'], ['AZ', 8, 'R'], ['AZ', 9, 'D'], ['AR', 1, 'R'], ['AR', 2, 'D'], ['AR', 3, 'R'], ['AR', 4, 'R'], ['CA', 1, 'R'], ['CA', 2, 'D'], ['CA', 3, 'D'], ['CA', 4, 'R'], ['CA', 6, 'D'], ['CA', 7, 'D'], ['CA', 8, 'R'], ['CA', 9, 'D'], ['CA', 10, 'R'], ['CA', 11, 'D'], ['CA', 12, 'D'], ['CA', 13, 'D'], ['CA', 14, 'D'], ['CA', 15, 'D'], ['CA', 16, 'D'], ['CA', 18, 'D'], ['CA', 19, 'D'], ['CA', 20, 'D'], ['CA', 21, 'R'], ['CA', 22, 'R'], ['CA', 23, 'R'], ['CA', 24, 'D'], ['CA', 25, 'R'], ['CA', 26, 'D'], ['CA', 27, 'D'], ['CA', 28, 'D'], ['CA', 29, 'D'], ['CA', 30, 'D'], ['CA', 31, 'D'], ['CA', 32, 'D'], ['CA', 33, 'D'], ['CA', 34, 'D'], ['CA', 35, 'D'], ['CA', 36, 'D'], ['CA', 37, 'D'], ['CA', 38, 'D'], ['CA', 39, 'R'], ['CA', 40, 'D'], ['CA', 41, 'D'], ['CA', 42, 'R'], ['CA', 43, 'D'], ['CA', 44, 'D'], ['CA', 45, 'R'], ['CA', 46, 'D'], ['CA', 47, 'D'], ['CA', 48, 'R'], ['CA', 49, 'D'], ['CA', 50, 'R'], ['CA', 51, 'D'], ['CA', 52, 'D'], ['CA', 53, 'D'], ['CO', 1, 'D'], ['CO', 2, 'D'], ['CO', 3, 'R'], ['CO', 4, 'R'], ['CO', 5, 'R'], ['CO', 6, 'R'], ['CO', 7, 'D'], ['CT', 1, 'D'], ['CT', 2, 'D'], ['CT', 3, 'D'], ['CT', 4, 'D'], ['CT', 5, 'D'], ['DE', 0, 'D'], ['FL', 1, 'R'], ['FL', 2, 'R'], ['FL', 3, 'R'], ['FL', 4, 'R'], ['FL', 5, 'D'], ['FL', 6, 'R'], ['FL', 7, 'D'], ['FL', 8, 'R'], ['FL', 9, 'D'], ['FL', 11, 'R'], ['FL', 12, 'R'], ['FL', 13, 'D'], ['FL', 15, 'R'], ['FL', 16, 'R'], ['FL', 17, 'R'], ['FL', 18, 'R'], ['FL', 19, 'R'], ['FL', 22, 'D'], ['FL', 23, 'D'], ['FL', 25, 'R'], ['FL', 26, 'R'], ['FL', 27, 'R'], ['GA', 1, 'R'], ['GA', 2, 'D'], ['GA', 3, 'R'], ['GA', 4, 'D'], ['GA', 6, 'R'], ['GA', 7, 'R'], ['GA', 10, 'R'], ['GA', 11, 'R'], ['GA', 12, 'R'], ['GA', 14, 'R'], ['HI', 1, 'D'], ['HI', 2, 'D'], ['ID', 1, 'R'], ['IL', 1, 'D'], ['IL', 2, 'D'], ['IL', 3, 'D'], ['IL', 4, 'D'], ['IL', 5, 'D'], ['IL', 6, 'R'], ['IL', 7, 'D'], ['IL', 8, 'D'], ['IL', 9, 'D'], ['IL', 10, 'D'], ['IL', 11, 'D'], ['IL', 12, 'R'], ['IL', 13, 'R'], ['IL', 14, 'R'], ['IL', 15, 'R'], ['IL', 16, 'R'], ['IL', 17, 'D'], ['IL', 18, 'R'], ['IN', 1, 'D'], ['IN', 2, 'R'], ['IN', 3, 'R'], ['IN', 4, 'R'], ['IN', 5, 'R'], ['IN', 6, 'D'], ['IN', 7, 'D'], ['IN', 8, 'R'], ['IN', 9, 'R'], ['IA', 1, 'R'], ['IA', 2, 'D'], ['IA', 3, 'R'], ['IA', 4, 'R'], ['KS', 1, 'R'], ['KS', 2, 'R'], ['KS', 3, 'R'], ['KS', 4, 'R'], ['KY', 1, 'R'], ['KY', 2, 'R'], ['KY', 3, 'D'], ['KY', 5, 'R'], ['KY', 6, 'D'], ['LA', 1, 'R'], ['LA', 2, 'D'], ['LA', 3, 'R'], ['LA', 4, 'R'], ['LA', 5, 'R'], ['ME', 1, 'D'], ['ME', 2, 'R'], ['MD', 1, 'D'], ['MD', 2, 'D'], ['MD', 3, 'D'], ['MD', 4, 'D'], ['MD', 5, 'D'], ['MD', 6, 'R'], ['MD', 7, 'D'], ['MD', 8, 'D'], ['MA', 2, 'D'], ['MA', 3, 'D'], ['MA', 5, 'D'], ['MA', 6, 'D'], ['MA', 9, 'D'], ['MI', 2, 'R'], ['MI', 3, 'R'], ['MI', 4, 'R'], ['MI', 5, 'D'], ['MI', 6, 'D'], ['MI', 7, 'R'], ['MI', 8, 'R'], ['MI', 9, 'D'], ['MI', 10, 'R'], ['MI', 11, 'R'], ['MI', 12, 'D'], ['MI', 13, 'D'], ['MN', 1, 'R'], ['MN', 2, 'D'], ['MN', 3, 'R'], ['MN', 4, 'D'], ['MN', 5, 'D'], ['MN', 6, 'R'], ['MN', 7, 'D'], ['MN', 8, 'R'], ['MS', 1, 'D'], ['MS', 2, 'D'], ['MS', 4, 'R'], ['MO', 1, 'D'], ['MO', 2, 'R'], ['MO', 3, 'R'], ['MO', 4, 'R'], ['MO', 5, 'D'], ['MO', 6, 'D'], ['MO', 7, 'R'], ['MO', 8, 'R'], ['MT', 0, 'R'], ['NE', 1, 'R'], ['NE', 2, 'R'], ['NE', 3, 'R'], ['NV', 1, 'D'], ['NV', 2, 'R'], ['NV', 3, 'D'], ['NV', 4, 'D'], ['NH', 1, 'D'], ['NJ', 1, 'D'], ['NJ', 2, 'R'], ['NJ', 3, 'R'], ['NJ', 4, 'R'], ['NJ', 5, 'D'], ['NJ', 6, 'D'], ['NJ', 7, 'R'], ['NJ', 8, 'D'], ['NJ', 9, 'D'], ['NJ', 10, 'D'], ['NJ', 11, 'R'], ['NJ', 12, 'D'], ['NM', 1, 'D'], ['NM', 2, 'R'], ['NM', 3, 'D'], ['NY', 1, 'R'], ['NY', 2, 'D'], ['NY', 3, 'D'], ['NY', 4, 'D'], ['NY', 6, 'D'], ['NY', 7, 'D'], ['NY', 8, 'D'], ['NY', 9, 'D'], ['NY', 10, 'D'], ['NY', 11, 'R'], ['NY', 12, 'D'], ['NY', 13, 'D'], ['NY', 14, 'D'], ['NY', 15, 'D'], ['NY', 17, 'D'], ['NY', 18, 'D'], ['NY', 19, 'D'], ['NY', 20, 'D'], ['NY', 21, 'R'], ['NY', 22, 'R'], ['NY', 24, 'R'], ['NY', 25, 'D'], ['NY', 26, 'D'], ['NY', 27, 'D'], ['NC', 1, 'D'], ['NC', 2, 'R'], ['NC', 4, 'D'], ['NC', 5, 'R'], ['NC', 6, 'R'], ['NC', 7, 'R'], ['NC', 8, 'R'], ['NC', 9, 'R'], ['NC', 10, 'R'], ['NC', 11, 'R'], ['NC', 12, 'D'], ['NC', 13, 'R'], ['ND', 0, 'R'], ['OH', 1, 'R'], ['OH', 2, 'R'], ['OH', 3, 'D'], ['OH', 4, 'R'], ['OH', 5, 'R'], ['OH', 6, 'R'], ['OH', 7, 'R'], ['OH', 8, 'R'], ['OH', 9, 'D'], ['OH', 10, 'R'], ['OH', 11, 'D'], ['OH', 12, 'R'], ['OH', 13, 'D'], ['OH', 14, 'R'], ['OH', 15, 'R'], ['OH', 16, 'R'], ['OK', 1, 'R'], ['OK', 2, 'R'], ['OK', 3, 'R'], ['OK', 4, 'R'], ['OK', 5, 'R'], ['OR', 1, 'D'], ['OR', 2, 'D'], ['OR', 3, 'D'], ['OR', 4, 'D'], ['OR', 5, 'D'], ['PA', 1, 'D'], ['PA', 2, 'D'], ['PA', 3, 'D'], ['PA', 4, 'R'], ['PA', 5, 'R'], ['PA', 6, 'D'], ['PA', 7, 'R'], ['PA', 8, 'R'], ['PA', 9, 'R'], ['PA', 10, 'R'], ['PA', 11, 'R'], ['PA', 12, 'D'], ['PA', 13, 'R'], ['PA', 14, 'D'], ['PA', 15, 'R'], ['PA', 16, 'R'], ['PA', 17, 'R'], ['RI', 1, 'D'], ['RI', 2, 'D'], ['SC', 1, 'R'], ['SC', 2, 'R'], ['SC', 3, 'R'], ['SC', 4, 'D'], ['SC', 5, 'R'], ['SC', 6, 'D'], ['SC', 7, 'R'], ['SD', 0, 'R'], ['TN', 1, 'R'], ['TN', 2, 'R'], ['TN', 3, 'R'], ['TN', 4, 'R'], ['TN', 5, 'D'], ['TN', 6, 'R'], ['TN', 7, 'R'], ['TN', 8, 'R'], ['TN', 9, 'D'], ['TX', 1, 'R'], ['TX', 2, 'D'], ['TX', 3, 'R'], ['TX', 4, 'R'], ['TX', 6, 'R'], ['TX', 7, 'R'], ['TX', 8, 'R'], ['TX', 9, 'D'], ['TX', 10, 'R'], ['TX', 11, 'R'], ['TX', 12, 'R'], ['TX', 13, 'R'], ['TX', 14, 'R'], ['TX', 15, 'D'], ['TX', 16, 'D'], ['TX', 17, 'R'], ['TX', 18, 'D'], ['TX', 19, 'R'], ['TX', 20, 'D'], ['TX', 21, 'R'], ['TX', 22, 'R'], ['TX', 23, 'R'], ['TX', 24, 'R'], ['TX', 25, 'R'], ['TX', 26, 'R'], ['TX', 27, 'R'], ['TX', 28, 'D'], ['TX', 29, 'D'], ['TX', 30, 'D'], ['TX', 31, 'R'], ['TX', 32, 'R'], ['TX', 34, 'D'], ['TX', 35, 'D'], ['TX', 36, 'R'], ['UT', 1, 'R'], ['UT', 2, 'R'], ['UT', 3, 'R'], ['UT', 4, 'R'], ['VT', 0, 'D'], ['VA', 1, 'R'], ['VA', 2, 'R'], ['VA', 4, 'D'], ['VA', 5, 'R'], ['VA', 6, 'R'], ['VA', 8, 'D'], ['VA', 9, 'R'], ['VA', 10, 'R'], ['VA', 11, 'D'], ['WA', 1, 'D'], ['WA', 2, 'D'], ['WA', 3, 'R'], ['WA', 4, 'R'], ['WA', 5, 'R'], ['WA', 6, 'D'], ['WA', 7, 'D'], ['WA', 8, 'D'], ['WA', 10, 'D'], ['WV', 1, 'R'], ['WV', 2, 'R'], ['WV', 3, 'R'], ['WI', 1, 'R'], ['WI', 3, 'D'], ['WI', 4, 'D'], ['WI', 5, 'R'], ['WI', 6, 'R'], ['WI', 7, 'R'], ['WI', 8, 'R'], ['WY', 0, 'R']]\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"2018erthush.csv\")\n",
    "\n",
    "dfident = what[['state', 'district', 'partyLEFT', 'partyRIGHT']]\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "\n",
    "trainingx = d1\n",
    "print(trainingx)\n",
    "\n",
    "trainingy = d2\n",
    "\n",
    "traininganswers = clf.predict(trainingx)\n",
    "\n",
    "zedata = pandas.DataFrame(traininganswers, columns=['won'])\n",
    "\n",
    "print(traininganswers)\n",
    "identwinners = dfident.join(zedata, lsuffix = 'SpentRIGHT')\n",
    "\n",
    "# print(identwinners)\n",
    "\n",
    "# realanswers = np.ravel(trainingy)\n",
    "\n",
    "lister = identwinners.values.tolist()\n",
    "# print(lister)\n",
    "\n",
    "for x in range(0,len(lister)):\n",
    "    if lister[x][4] == 0:\n",
    "        new = lister[x]\n",
    "        new.pop()\n",
    "        new.pop()\n",
    "        lister[x] = new\n",
    "        if lister[x][2] == 'REP':\n",
    "            lister[x][2] = 'R'\n",
    "        elif lister[x][2] == 'DEM' or lister[x][2] == 'DFL':\n",
    "            lister[x][2] = 'D'\n",
    "        else:\n",
    "            print(\"\\n \\n \\n \\n ???? \\n \\n \\n ???? FAIL \\n \\n \\n\")\n",
    "            print(lister[x])\n",
    "    elif lister[x][4] == 1:\n",
    "        new = lister[x]\n",
    "        new.pop()\n",
    "        new.pop(2)\n",
    "        lister[x] = new\n",
    "        if lister[x][2] == 'REP':\n",
    "            lister[x][2] = 'R'\n",
    "        elif lister[x][2] == 'DEM' or lister[x][2] == 'DFL':\n",
    "            lister[x][2] = 'D'\n",
    "        else:\n",
    "            print(\"\\n \\n \\n \\n ???? \\n \\n \\n ???? FAIL \\n \\n \\n\")\n",
    "            print(lister[x])\n",
    "    else:\n",
    "        print(\"#### \\n \\n \\n \\n ##### \\n \\n \\n ### FAIL \\n \\n \\n\")\n",
    "\n",
    "print(lister)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# print(\"Below is our training data analysis\")\n",
    "# summer = sum(realanswers)\n",
    "# print(summer)\n",
    "\n",
    "# numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "# print(numcorrect)\n",
    "# print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "# print(\"\\n \\n \\n\")\n",
    "\n",
    "\n",
    "# # testing score\n",
    "# score = metrics.f1_score(trainingy, traininganswers, pos_label=list(set(trainingy)))\n",
    "# training score\n",
    "# score_test = metrics.f1_score(testingy, testinganswers, pos_label=list(set(testingy)))\n",
    "\n",
    "# print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AK', 0, 'R'], ['AL', 1, 'R'], ['AL', 2, 'R'], ['AL', 3, 'R'], ['AL', 4, 'R'], ['AL', 5, 'R'], ['AL', 6, 'R'], ['AL', 7, 'D'], ['AR', 1, 'R'], ['AR', 2, 'R'], ['AR', 3, 'R'], ['AR', 4, 'R'], ['AZ', 1, 'D'], ['AZ', 2, 'D'], ['AZ', 3, 'D'], ['AZ', 4, 'R'], ['AZ', 5, 'R'], ['AZ', 6, 'R'], ['AZ', 7, 'D'], ['AZ', 8, 'R'], ['AZ', 9, 'D'], ['CA', 1, 'R'], ['CA', 2, 'D'], ['CA', 3, 'D'], ['CA', 4, 'R'], ['CA', 5, 'D'], ['CA', 6, 'D'], ['CA', 7, 'D'], ['CA', 8, 'R'], ['CA', 9, 'D'], ['CA', 10, 'D'], ['CA', 11, 'D'], ['CA', 12, 'D'], ['CA', 13, 'D'], ['CA', 14, 'D'], ['CA', 15, 'D'], ['CA', 16, 'D'], ['CA', 17, 'D'], ['CA', 18, 'D'], ['CA', 19, 'D'], ['CA', 20, 'D'], ['CA', 21, 'R'], ['CA', 22, 'R'], ['CA', 23, 'R'], ['CA', 24, 'D'], ['CA', 25, 'D'], ['CA', 26, 'D'], ['CA', 27, 'D'], ['CA', 28, 'D'], ['CA', 29, 'D'], ['CA', 30, 'D'], ['CA', 31, 'D'], ['CA', 32, 'D'], ['CA', 33, 'D'], ['CA', 34, 'D'], ['CA', 35, 'D'], ['CA', 36, 'D'], ['CA', 37, 'D'], ['CA', 38, 'D'], ['CA', 39, 'D'], ['CA', 40, 'D'], ['CA', 41, 'D'], ['CA', 42, 'R'], ['CA', 43, 'D'], ['CA', 44, 'D'], ['CA', 45, 'D'], ['CA', 46, 'D'], ['CA', 47, 'D'], ['CA', 48, 'D'], ['CA', 49, 'D'], ['CA', 50, 'R'], ['CA', 51, 'D'], ['CA', 52, 'D'], ['CA', 53, 'D'], ['CO', 1, 'D'], ['CO', 2, 'D'], ['CO', 3, 'R'], ['CO', 4, 'R'], ['CO', 5, 'R'], ['CO', 6, 'D'], ['CO', 7, 'D'], ['CT', 1, 'D'], ['CT', 2, 'D'], ['CT', 3, 'D'], ['CT', 4, 'D'], ['CT', 5, 'D'], ['DE', 1, 'D'], ['FL', 1, 'R'], ['FL', 2, 'R'], ['FL', 3, 'R'], ['FL', 4, 'R'], ['FL', 5, 'D'], ['FL', 6, 'R'], ['FL', 7, 'D'], ['FL', 8, 'R'], ['FL', 9, 'D'], ['FL', 10, 'D'], ['FL', 11, 'R'], ['FL', 12, 'R'], ['FL', 13, 'D'], ['FL', 14, 'D'], ['FL', 15, 'R'], ['FL', 16, 'R'], ['FL', 17, 'R'], ['FL', 18, 'R'], ['FL', 19, 'R'], ['FL', 20, 'D'], ['FL', 21, 'D'], ['FL', 22, 'D'], ['FL', 23, 'D'], ['FL', 24, 'D'], ['FL', 25, 'R'], ['FL', 26, 'R'], ['FL', 27, 'D'], ['GA', 1, 'R'], ['GA', 2, 'D'], ['GA', 3, 'R'], ['GA', 4, 'D'], ['GA', 5, 'D'], ['GA', 6, 'R'], ['GA', 7, 'R'], ['GA', 8, 'R'], ['GA', 9, 'R'], ['GA', 10, 'R'], ['GA', 11, 'R'], ['GA', 12, 'R'], ['GA', 13, 'D'], ['GA', 14, 'R'], ['HI', 1, 'D'], ['HI', 2, 'D'], ['IA', 1, 'D'], ['IA', 2, 'D'], ['IA', 3, 'D'], ['IA', 4, 'R'], ['ID', 1, 'R'], ['ID', 2, 'R'], ['IL', 1, 'D'], ['IL', 2, 'D'], ['IL', 3, 'D'], ['IL', 4, 'D'], ['IL', 5, 'D'], ['IL', 6, 'R'], ['IL', 7, 'D'], ['IL', 8, 'D'], ['IL', 9, 'D'], ['IL', 10, 'D'], ['IL', 11, 'D'], ['IL', 12, 'R'], ['IL', 13, 'R'], ['IL', 14, 'R'], ['IL', 15, 'R'], ['IL', 16, 'R'], ['IL', 17, 'D'], ['IL', 18, 'R'], ['IN', 1, 'D'], ['IN', 2, 'R'], ['IN', 3, 'R'], ['IN', 4, 'R'], ['IN', 5, 'R'], ['IN', 6, 'R'], ['IN', 7, 'D'], ['IN', 8, 'R'], ['IN', 9, 'R'], ['KS', 1, 'R'], ['KS', 2, 'D'], ['KS', 3, 'D'], ['KS', 4, 'R'], ['KY', 1, 'R'], ['KY', 2, 'R'], ['KY', 3, 'D'], ['KY', 4, 'R'], ['KY', 5, 'R'], ['KY', 6, 'R'], ['LA', 1, 'R'], ['LA', 2, 'D'], ['LA', 3, 'R'], ['LA', 4, 'R'], ['LA', 5, 'R'], ['LA', 6, 'R'], ['MA', 1, 'D'], ['MA', 2, 'D'], ['MA', 3, 'D'], ['MA', 4, 'D'], ['MA', 5, 'D'], ['MA', 6, 'D'], ['MA', 7, 'D'], ['MA', 8, 'D'], ['MA', 9, 'D'], ['MD', 1, 'R'], ['MD', 2, 'D'], ['MD', 3, 'D'], ['MD', 4, 'D'], ['MD', 5, 'D'], ['MD', 6, 'D'], ['MD', 7, 'D'], ['MD', 8, 'D'], ['ME', 1, 'D'], ['ME', 2, 'R'], ['MI', 1, 'R'], ['MI', 2, 'R'], ['MI', 3, 'R'], ['MI', 4, 'R'], ['MI', 5, 'D'], ['MI', 6, 'R'], ['MI', 7, 'R'], ['MI', 8, 'D'], ['MI', 9, 'D'], ['MI', 10, 'R'], ['MI', 11, 'D'], ['MI', 12, 'D'], ['MI', 13, 'D'], ['MI', 14, 'D'], ['MN', 1, 'R'], ['MN', 2, 'D'], ['MN', 3, 'D'], ['MN', 4, 'D'], ['MN', 5, 'D'], ['MN', 6, 'R'], ['MN', 7, 'D'], ['MN', 8, 'R'], ['MO', 1, 'D'], ['MO', 2, 'R'], ['MO', 3, 'R'], ['MO', 4, 'R'], ['MO', 5, 'D'], ['MO', 6, 'R'], ['MO', 7, 'R'], ['MO', 8, 'R'], ['MS', 1, 'R'], ['MS', 2, 'D'], ['MS', 3, 'R'], ['MS', 4, 'R'], ['MT', 1, 'R'], ['NC', 1, 'D'], ['NC', 2, 'R'], ['NC', 3, 'R'], ['NC', 4, 'D'], ['NC', 5, 'R'], ['NC', 6, 'R'], ['NC', 7, 'R'], ['NC', 8, 'R'], ['NC', 9, 'D'], ['NC', 10, 'R'], ['NC', 11, 'R'], ['NC', 12, 'D'], ['NC', 13, 'R'], ['ND', 1, 'R'], ['NE', 1, 'R'], ['NE', 2, 'R'], ['NE', 3, 'R'], ['NH', 1, 'D'], ['NH', 2, 'D'], ['NJ', 1, 'D'], ['NJ', 2, 'D'], ['NJ', 3, 'D'], ['NJ', 4, 'R'], ['NJ', 5, 'D'], ['NJ', 6, 'D'], ['NJ', 7, 'D'], ['NJ', 8, 'D'], ['NJ', 9, 'D'], ['NJ', 10, 'D'], ['NJ', 11, 'D'], ['NJ', 12, 'D'], ['NM', 1, 'D'], ['NM', 2, 'R'], ['NM', 3, 'D'], ['NV', 1, 'D'], ['NV', 2, 'R'], ['NV', 3, 'D'], ['NV', 4, 'D'], ['NY', 1, 'R'], ['NY', 2, 'R'], ['NY', 3, 'D'], ['NY', 4, 'D'], ['NY', 5, 'D'], ['NY', 6, 'D'], ['NY', 7, 'D'], ['NY', 8, 'D'], ['NY', 9, 'D'], ['NY', 10, 'D'], ['NY', 11, 'R'], ['NY', 12, 'D'], ['NY', 13, 'D'], ['NY', 14, 'D'], ['NY', 15, 'D'], ['NY', 16, 'D'], ['NY', 17, 'D'], ['NY', 18, 'D'], ['NY', 19, 'D'], ['NY', 20, 'D'], ['NY', 21, 'R'], ['NY', 22, 'D'], ['NY', 23, 'R'], ['NY', 24, 'R'], ['NY', 25, 'D'], ['NY', 26, 'D'], ['NY', 27, 'R'], ['OH', 1, 'R'], ['OH', 2, 'R'], ['OH', 3, 'D'], ['OH', 4, 'R'], ['OH', 5, 'R'], ['OH', 6, 'R'], ['OH', 7, 'R'], ['OH', 8, 'R'], ['OH', 9, 'D'], ['OH', 10, 'R'], ['OH', 11, 'D'], ['OH', 12, 'R'], ['OH', 13, 'D'], ['OH', 14, 'R'], ['OH', 15, 'R'], ['OH', 16, 'R'], ['OK', 1, 'R'], ['OK', 2, 'R'], ['OK', 3, 'R'], ['OK', 4, 'R'], ['OK', 5, 'R'], ['OR', 1, 'D'], ['OR', 2, 'R'], ['OR', 3, 'D'], ['OR', 4, 'D'], ['OR', 5, 'D'], ['PA', 1, 'R'], ['PA', 2, 'D'], ['PA', 3, 'D'], ['PA', 4, 'D'], ['PA', 5, 'D'], ['PA', 6, 'D'], ['PA', 7, 'D'], ['PA', 8, 'D'], ['PA', 9, 'R'], ['PA', 10, 'R'], ['PA', 11, 'R'], ['PA', 12, 'R'], ['PA', 13, 'R'], ['PA', 14, 'R'], ['PA', 15, 'R'], ['PA', 16, 'R'], ['PA', 17, 'D'], ['PA', 18, 'D'], ['RI', 1, 'D'], ['RI', 2, 'D'], ['SC', 1, 'R'], ['SC', 2, 'R'], ['SC', 3, 'R'], ['SC', 4, 'R'], ['SC', 5, 'R'], ['SC', 6, 'D'], ['SC', 7, 'R'], ['SD', 1, 'R'], ['TN', 1, 'R'], ['TN', 2, 'R'], ['TN', 3, 'R'], ['TN', 4, 'R'], ['TN', 5, 'D'], ['TN', 6, 'R'], ['TN', 7, 'R'], ['TN', 8, 'R'], ['TN', 9, 'D'], ['TX', 1, 'R'], ['TX', 2, 'R'], ['TX', 3, 'R'], ['TX', 4, 'R'], ['TX', 5, 'R'], ['TX', 6, 'R'], ['TX', 7, 'R'], ['TX', 8, 'R'], ['TX', 9, 'D'], ['TX', 10, 'R'], ['TX', 11, 'R'], ['TX', 12, 'R'], ['TX', 13, 'R'], ['TX', 14, 'R'], ['TX', 15, 'D'], ['TX', 16, 'D'], ['TX', 17, 'R'], ['TX', 18, 'D'], ['TX', 19, 'R'], ['TX', 20, 'D'], ['TX', 21, 'R'], ['TX', 22, 'R'], ['TX', 23, 'R'], ['TX', 24, 'R'], ['TX', 25, 'R'], ['TX', 26, 'R'], ['TX', 27, 'R'], ['TX', 28, 'D'], ['TX', 29, 'D'], ['TX', 30, 'D'], ['TX', 31, 'R'], ['TX', 32, 'R'], ['TX', 33, 'D'], ['TX', 34, 'D'], ['TX', 35, 'D'], ['TX', 36, 'R'], ['UT', 1, 'R'], ['UT', 2, 'R'], ['UT', 3, 'R'], ['UT', 4, 'R'], ['VA', 1, 'R'], ['VA', 2, 'R'], ['VA', 3, 'D'], ['VA', 4, 'D'], ['VA', 5, 'R'], ['VA', 6, 'R'], ['VA', 7, 'R'], ['VA', 8, 'D'], ['VA', 9, 'R'], ['VA', 10, 'D'], ['VA', 11, 'D'], ['VT', 1, 'D'], ['WA', 1, 'D'], ['WA', 2, 'D'], ['WA', 3, 'R'], ['WA', 4, 'R'], ['WA', 5, 'R'], ['WA', 6, 'D'], ['WA', 7, 'D'], ['WA', 8, 'D'], ['WA', 9, 'D'], ['WA', 10, 'D'], ['WI', 1, 'R'], ['WI', 2, 'D'], ['WI', 3, 'D'], ['WI', 4, 'D'], ['WI', 5, 'R'], ['WI', 6, 'R'], ['WI', 7, 'R'], ['WI', 8, 'R'], ['WV', 1, 'R'], ['WV', 2, 'R'], ['WV', 3, 'R'], ['WY', 1, 'R']]\n",
      "350 400\n"
     ]
    }
   ],
   "source": [
    "comparison = pandas.read_csv(\"newbforec.csv\")\n",
    "\n",
    "cutdown = comparison[['state', 'district', 'party']]\n",
    "doneso = cutdown.values.tolist()\n",
    "print(doneso)\n",
    "\n",
    "summy = 0\n",
    "tote = 0\n",
    "for x in doneso:\n",
    "    if x in lister:\n",
    "        summy += 1\n",
    "        tote += 1\n",
    "    else:\n",
    "        tote +=1\n",
    "        \n",
    "print(summy,tote)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
