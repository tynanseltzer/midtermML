{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n",
      "['AL3', 'AL4', 'AL5', 'AL6', 'AL7', 'AL1', 'AZ6', 'AZ7', 'AR3', 'CA4', 'CA5', 'CA6', 'CA9', 'CA11', 'CA12', 'CA13', 'CA14', 'CA15', 'CA16', 'CA17', 'CA18', 'CA20', 'CA21', 'CA22', 'CA27', 'CA28', 'CA29', 'CA30', 'CA32', 'CA33', 'CA34', 'CA35', 'CA37', 'CA38', 'CA42', 'CA43', 'CA45', 'CA47', 'CA48', 'CA49', 'CA50', 'CA51', 'CA53', 'CO1', 'CO2', 'CO5', 'CT1', 'CT2', 'CT3', 'FL1', 'FL4', 'FL6', 'FL7', 'FL8', 'FL12', 'FL13', 'FL15', 'FL16', 'FL20', 'FL23', 'FL24', 'GA1', 'GA2', 'GA3', 'GA4', 'GA5', 'GA6', 'GA7', 'GA9', 'GA10', 'GA11', 'GA13', 'HI2', 'ID2', 'IL1', 'IL2', 'IL3', 'IL4', 'IL5', 'IL6', 'IL7', 'IL9', 'IL15', 'IL16', 'IL18', 'IN1', 'IN3', 'IN4', 'IN5', 'IN6', 'IN7', 'IA4', 'KS1', 'KS2', 'KS4', 'KY1', 'KY2', 'KY3', 'KY4', 'KY5', 'LA1', 'LA2', 'LA5', 'ME1', 'MD2', 'MD3', 'MD4', 'MD5', 'MD7', 'MD8', 'MA1', 'MA2', 'MA3', 'MA4', 'MA5', 'MA7', 'MA8', 'MA9', 'MI2', 'MI3', 'MI4', 'MI5', 'MI6', 'MI8', 'MI10', 'MI12', 'MI13', 'MI14', 'MN3', 'MN4', 'MN5', 'MS1', 'MS2', 'MS3', 'MO1', 'MO2', 'MO6', 'MO7', 'MO8', 'NE1', 'NE3', 'NV1', 'NV2', 'NJ1', 'NJ2', 'NJ4', 'NJ5', 'NJ6', 'NJ8', 'NJ9', 'NJ10', 'NJ11', 'NJ12', 'NM1', 'NM2', 'NM3', 'NY2', 'NY6', 'NY7', 'NY8', 'NY9', 'NY12', 'NY14', 'NY15', 'NY16', 'NY17', 'NC1', 'NC3', 'NC4', 'NC5', 'NC6', 'NC9', 'NC10', 'NC12', 'OH2', 'OH4', 'OH5', 'OH7', 'OH8', 'OH9', 'OH11', 'OH12', 'OH13', 'OH14', 'OK1', 'OK3', 'OK4', 'OK5', 'OR2', 'OR3', 'PA1', 'PA2', 'PA5', 'PA6', 'PA9', 'PA10', 'PA13', 'PA14', 'PA15', 'PA17', 'PA18', 'RI2', 'SC1', 'SC3', 'SC4', 'SC5', 'SC6', 'TN1', 'TN2', 'TN3', 'TN4', 'TN5', 'TN6', 'TN7', 'TN8', 'TN9', 'TX1', 'TX2', 'TX3', 'TX4', 'TX5', 'TX6', 'TX7', 'TX8', 'TX9', 'TX10', 'TX11', 'TX12', 'TX13', 'TX14', 'TX15', 'TX16', 'TX17', 'TX18', 'TX19', 'TX20', 'TX21', 'TX22', 'TX24', 'TX26', 'TX28', 'TX29', 'TX30', 'TX31', 'TX32', 'UT1', 'UT3', 'VA1', 'VA3', 'VA4', 'VA6', 'VA7', 'VA8', 'VA10', 'WA1', 'WA2', 'WA4', 'WA5', 'WA6', 'WA7', 'WA9', 'WV2', 'WI1', 'WI2', 'WI3', 'WI4', 'WI5', 'WI6']\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "import csv\n",
    "import ast\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "df1 = pandas.read_csv(\"time2match.csv\", sep = \"#\", engine = \"python\")\n",
    "\n",
    "listed = df1.values.tolist()\n",
    "\n",
    "tracker = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for j in listed:\n",
    "    tracker[j[1]][j[2]][j[3]].append(j[6])\n",
    "\n",
    "# print(tracker)\n",
    "\n",
    "lsted = []\n",
    "\n",
    "uncontested = defaultdict(lambda: defaultdict(lambda: defaultdict(bool)))\n",
    "bro = defaultdict(lambda: defaultdict(bool))\n",
    "unlist = []\n",
    "conlist = []\n",
    "\n",
    "for i in tracker.keys():\n",
    "    for j in tracker[i].keys():\n",
    "        for k in tracker[i][j].keys():\n",
    "            yearsdom = 0\n",
    "            for share in tracker[i][j][k]:\n",
    "#                 print(share)\n",
    "                if share > 0.55:\n",
    "                    yearsdom += 1\n",
    "                else:\n",
    "                    continue\n",
    "            if yearsdom >= 4:\n",
    "                uncontested[i][j][k] = True\n",
    "                unlist.append((i,j))\n",
    "            else:\n",
    "                bro[i][j] = False\n",
    "                conlist.append((i,j))\n",
    "                    \n",
    "# print(uncontested)\n",
    "# print(unlist)\n",
    "print(435 - len(unlist))\n",
    "# print(conlist)\n",
    "# print(len(list(set(conlist))))\n",
    "\n",
    "combinelist = list()\n",
    "for x,y in unlist:\n",
    "    combinelist.append(str(x)+str(y))\n",
    "\n",
    "print(combinelist)\n",
    "print(len(combinelist))\n",
    "    \n",
    "oobscores =[]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      filler  filler.1  filler.2                     filler.3 state  district  \\\n",
      "0          0         4         5                   BONNER, JO    AL         1   \n",
      "1          0         9         8        BRIGHT SR, BOBBY NEAL    AL         2   \n",
      "2          0        14        17              ROGERS, MICHAEL    AL         3   \n",
      "3          0        18        24                   BROOKS, MO    AL         5   \n",
      "4          0        29        30  CHAMBERLAIN, DONALD NORWOOD    AL         7   \n",
      "5          0         1         2         CRAWFORD, HARRY T JR    AK         0   \n",
      "6          0        81        86  GOSAR, PAUL ANTHONY ANTHONY    AZ         1   \n",
      "7          0        93        97                FRANKS, TRENT    AZ         2   \n",
      "8          0       104       112                 HULBURD, JON    AZ         3   \n",
      "9          0       114       118           CONTRERAS, JANET L    AZ         4   \n",
      "10         0       123       130           MITCHELL, HARRY E.    AZ         5   \n",
      "11         0       131       134              FLAKE, JEFF MR.    AZ         6   \n",
      "12         0       138       146             GRIJALVA, RAUL M    AZ         7   \n",
      "13         0       147       151          GIFFORDS, GABRIELLE    AZ         8   \n",
      "14         0        42        50                 CAUSEY, CHAD    AR         1   \n",
      "15         0        53        56           ELLIOTT, JOYCE ANN    AR         2   \n",
      "16         0        69        67      WHITAKER, DAVID JEFFREY    AR         3   \n",
      "17         0        73        75            RANKIN, BETH ANNE    AR         4   \n",
      "18         0       155       158               HANKS, LOREN L    CA         1   \n",
      "19         0       159       163                HERGER, WALLY    CA         2   \n",
      "20         0       163       171                BERA, AMERISH    CA         3   \n",
      "21         0       173       175                CURTIS, CLINT    CA         4   \n",
      "22         0       178       179                MATSUI, DORIS    CA         5   \n",
      "23         0       182       188            JUDD, JAMES O JR.    CA         6   \n",
      "24         0       190       192               MILLER, GEORGE    CA         7   \n",
      "25         0       195       200                 DENNIS, JOHN    CA         8   \n",
      "26         0       202       201             HASIMOTO, GERALD    CA         9   \n",
      "27         0       209       220          CLIFT, GARY WILLIAM    CA        10   \n",
      "28         0       227       223                HARMER, DAVID    CA        11   \n",
      "29         0       232       235          MOLONEY, MICHAEL J.    CA        12   \n",
      "...      ...       ...       ...                          ...   ...       ...   \n",
      "1627       0      9971      9972          BROWN, SHAUN DENISE    VA         2   \n",
      "1628       0      9974      9976             SCOTT, ROBERT C.    VA         3   \n",
      "1629       0      9979      9981   MCEACHIN, ASTON DONALD MR.    VA         4   \n",
      "1630       0      9985      9987       DITTMAR, JANE DESIMONE    VA         5   \n",
      "1631       0      9989      9993                  DEGNER, KAI    VA         6   \n",
      "1632       0      9993      9997        BEDELL, EILEEN MCNEIL    VA         7   \n",
      "1633       0      9998     10001   BEYER, DONALD STERNOFF JR.    VA         8   \n",
      "1634       0     10004     10006           GRIFFITH, H MORGAN    VA         9   \n",
      "1635       0     10007     10009               BENNETT, LUANN    VA        10   \n",
      "1636       0     10019     10026             DELBENE, SUZAN K    WA         1   \n",
      "1637       0     10026     10030              HENNEMANN, MARC    WA         2   \n",
      "1638       0     10034     10039       HERRERA BEUTLER, JAIME    WA         3   \n",
      "1639       0     10040     10041                DIDIER, CLINT    WA         4   \n",
      "1640       0     10049     10050      MCMORRIS RODGERS, CATHY    WA         5   \n",
      "1641       0     10053     10054          BLOOM, TODD ANTHONY    WA         6   \n",
      "1642       0     10060     10063             JAYAPAL, PRAMILA    WA         7   \n",
      "1643       0     10072     10077               REICHERT, DAVE    WA         8   \n",
      "1644       0     10077     10078      BASLER, DOUGLAS MICHAEL    WA         9   \n",
      "1645       0     10084     10086                 HECK, DENNIS    WA        10   \n",
      "1646       0     10125     10126              MANYPENNY, MIKE    WV         1   \n",
      "1647       0     10127     10131             HUNT, MARK ALLEN    WV         2   \n",
      "1648       0     10134     10135          DETCH, MATTHEW PAUL    WV         3   \n",
      "1649       0     10090     10088                RYAN, PAUL D.    WI         1   \n",
      "1650       0     10093     10095                  POCAN, MARK    WI         2   \n",
      "1651       0     10100     10102                MOORE, GWEN S    WI         4   \n",
      "1652       0     10104     10105             PENEBAKER, KHARY    WI         5   \n",
      "1653       0     10107     10110           GROTHMAN, GLENN S.    WI         6   \n",
      "1654       0     10111     10118                  DUFFY, SEAN    WI         7   \n",
      "1655       0     10118     10124      GALLAGHER, MICHAEL JOHN    WI         8   \n",
      "1656       0     10139     10153       CHENEY, ELIZABETH MRS.    WY         0   \n",
      "\n",
      "     candpartyLEFT  filler.4  filler.5  filler.6      ...       filler.29  \\\n",
      "0              REP      2010  0.390789      20.5      ...           False   \n",
      "1              DEM      2010  0.407024      19.3      ...           False   \n",
      "2              REP      2010  0.456925      19.5      ...            True   \n",
      "3              REP      2010  0.369057      26.7      ...            True   \n",
      "4              REP      2010  0.522854      17.1      ...            True   \n",
      "5              DEM      2010       NaN      27.0      ...           False   \n",
      "6              REP      2010  0.441068      19.2      ...            True   \n",
      "7              REP      2010  0.439815      21.9      ...            True   \n",
      "8              DEM      2010  0.445441      33.9      ...           False   \n",
      "9              REP      2010  0.445441      13.6      ...            True   \n",
      "10             DEM      2010  0.445441      43.6      ...           False   \n",
      "11             REP      2010  0.443988      28.5      ...            True   \n",
      "12             DEM      2010  0.463122      15.3      ...           False   \n",
      "13             DEM      2010  0.502409      34.1      ...           False   \n",
      "14             DEM      2010  0.386129      13.7      ...           False   \n",
      "15             DEM      2010  0.447858      25.8      ...           False   \n",
      "16             DEM      2010  0.343305      21.3      ...           False   \n",
      "17             REP      2010  0.404828      15.0      ...            True   \n",
      "18             REP      2010  0.704198      28.4      ...            True   \n",
      "19             REP      2010  0.491595      20.4      ...            True   \n",
      "20             DEM      2010  0.595003      29.0      ...           False   \n",
      "21             DEM      2010  0.534013      29.6      ...           False   \n",
      "22             DEM      2010  0.595244      25.4      ...           False   \n",
      "23             REP      2010  0.768708      40.5      ...            True   \n",
      "24             DEM      2010  0.681005      25.5      ...           False   \n",
      "25             REP      2010  0.861222      50.1      ...            True   \n",
      "26             REP      2010  0.800612      42.6      ...            True   \n",
      "27             REP      2010  0.695085      39.4      ...            True   \n",
      "28             REP      2010  0.715547      32.2      ...            True   \n",
      "29             REP      2010  0.812385      46.3      ...            True   \n",
      "...            ...       ...       ...       ...      ...             ...   \n",
      "1627           DEM      2016  0.585814      33.2      ...           False   \n",
      "1628           DEM      2016  0.635135      25.3      ...           False   \n",
      "1629           DEM      2016  0.498744      27.6      ...           False   \n",
      "1630           DEM      2016  0.457956      26.5      ...           False   \n",
      "1631           DEM      2016  0.394125      26.6      ...           False   \n",
      "1632           DEM      2016  0.511733      37.3      ...           False   \n",
      "1633           DEM      2016  0.634034      61.1      ...           False   \n",
      "1634           REP      2016  0.354572      19.7      ...            True   \n",
      "1635           DEM      2016  0.558579      54.4      ...           False   \n",
      "1636           DEM      2016  0.663208      41.6      ...           False   \n",
      "1637           REP      2016  0.656823      30.0      ...            True   \n",
      "1638           REP      2016  0.524482      23.8      ...            True   \n",
      "1639           REP      2016  0.389063      19.6      ...           False   \n",
      "1640           REP      2016  0.443325      28.4      ...            True   \n",
      "1641           REP      2016  0.556226      28.6      ...            True   \n",
      "1642           DEM      2016  0.704080      58.2      ...            True   \n",
      "1643           REP      2016  0.663278      33.0      ...            True   \n",
      "1644           REP      2016  0.657721      40.3      ...            True   \n",
      "1645           DEM      2016       NaN      26.7      ...           False   \n",
      "1646           DEM      2016  0.363358      21.8      ...           False   \n",
      "1647           DEM      2016  0.387459      21.2      ...           False   \n",
      "1648           DEM      2016  0.332106      15.6      ...           False   \n",
      "1649           REP      2016  0.553692      27.3      ...            True   \n",
      "1650           DEM      2016  0.639056      40.2      ...           False   \n",
      "1651           DEM      2016  0.674361      27.1      ...           False   \n",
      "1652           DEM      2016  0.518769      35.7      ...           False   \n",
      "1653           REP      2016  0.471164      25.2      ...            True   \n",
      "1654           REP      2016  0.505058      22.0      ...            True   \n",
      "1655           REP      2016  0.480119      24.9      ...            True   \n",
      "1656           REP      2016       NaN      26.0      ...            True   \n",
      "\n",
      "      filler.30  filler.31  filler.32  filler.33  filler.34  RaisedRIGHT  \\\n",
      "0           4.0    41172.0          4        NaN   79.80000          NaN   \n",
      "1           0.0    40567.0        161      48.55   80.00000   1253557.11   \n",
      "2           4.0    35452.0         33        100   79.34211      8750.00   \n",
      "3           4.0    45856.0         33      61.67   77.90000    929084.38   \n",
      "4           NaN    31047.0        148       36.8   78.90000   1739832.56   \n",
      "5           4.0    64576.0         42      70.35   78.10000   1001015.37   \n",
      "6           0.0    40854.0        100        100   77.15333   1957921.74   \n",
      "7           4.0    47963.0          6        100   77.50000     21020.76   \n",
      "8           1.0    52327.0         44      22.05   76.50000   2653070.20   \n",
      "9           0.0    31498.0         40        100   77.19474   1014290.67   \n",
      "10          0.0    55876.0          4      37.23   77.49048   1732731.48   \n",
      "11          1.0    56691.0         13        100   78.70000         0.00   \n",
      "12          0.0    39777.0        196      50.95   76.10000    732607.51   \n",
      "13          NaN    49825.0         66      48.31   77.70000   1692503.57   \n",
      "14          4.0    33862.0         14      71.79   77.20000   1263738.21   \n",
      "15          0.0    43685.0          6      61.69   77.00000   1855578.10   \n",
      "16          4.0    40674.0        159      31.18   77.08667    717879.42   \n",
      "17          NaN    35312.0          1        100   79.30000   2426280.46   \n",
      "18          1.0    48003.0          1        100   78.88947   1912475.17   \n",
      "19          0.0    42249.0          5        100   78.17143    137553.75   \n",
      "20          0.0    61853.0         12        100   77.40000   2025541.46   \n",
      "21          4.0    60629.0         20      78.48   78.30000   1871673.55   \n",
      "22          4.0    43953.0         27      56.68   77.10000     19141.29   \n",
      "23          0.0    64391.0        164      99.99   77.20000    846466.17   \n",
      "24          4.0    61315.0         16      57.07   77.50000    108490.78   \n",
      "25          4.0    69926.0         78        100   76.60000   2597318.64   \n",
      "26          4.0    53286.0         76        100   76.95333   1156048.78   \n",
      "27          4.0    75457.0          6        100   76.90000   1764504.06   \n",
      "28          0.0    72075.0         63        100   76.30000   3208078.03   \n",
      "29          4.0    82337.0        500        100   76.95789   1060952.74   \n",
      "...         ...        ...        ...        ...        ...          ...   \n",
      "1627        0.0    66953.0         17        NaN   67.30000    839484.99   \n",
      "1628        4.0    51794.0        115        NaN   69.10000     74317.31   \n",
      "1629        0.0    54607.0          1        NaN   68.33333    146692.70   \n",
      "1630        0.0    52237.0         20        NaN   71.50000    647091.95   \n",
      "1631        3.0    50061.0          7        100   71.70000   1777614.99   \n",
      "1632        0.0    73580.0          4      55.53   71.83158   1259401.14   \n",
      "1633        0.0   100719.0         23        NaN   72.06190     81179.34   \n",
      "1634        0.0    41698.0         73        NaN   72.30000    102048.52   \n",
      "1635        0.0   120384.0         76      53.91   72.60000   5279626.17   \n",
      "1636        4.0    91018.0          7      15.61   72.70000     27950.18   \n",
      "1637        4.0    65187.0         16      55.59   74.00000    957187.22   \n",
      "1638        0.0    61304.0          5        NaN   73.32000    112575.05   \n",
      "1639        0.0    52484.0         12      25.59   66.30000     75051.99   \n",
      "1640        4.0    51697.0         11      28.65   66.90000    343465.59   \n",
      "1641        4.0    60616.0         73       58.7   67.38947   2137491.76   \n",
      "1642        4.0    84579.0        500        NaN   68.02381    468435.42   \n",
      "1643        0.0    79516.0         80        NaN   68.50000     31422.45   \n",
      "1644        0.0    75407.0        500         64   68.30000    921266.54   \n",
      "1645        NaN    62206.0          5        NaN   73.10000         0.00   \n",
      "1646        0.0    45611.0          4        100   68.30000   1147264.04   \n",
      "1647        3.0    48358.0        106      36.02   68.00000   1750907.73   \n",
      "1648        NaN    37728.0        500        100   69.60000   1269625.19   \n",
      "1649        0.0    61589.0         25        NaN   82.00000     23748.92   \n",
      "1650        0.0    65589.0         12      99.84   80.90000     38282.99   \n",
      "1651        0.0    41816.0          7        NaN   74.80476          NaN   \n",
      "1652        0.0    68162.0        500      99.81   72.00000    384926.81   \n",
      "1653        0.0    57129.0         19        NaN   72.80000    191932.26   \n",
      "1654        0.0    51738.0         15        NaN   71.90000    116276.39   \n",
      "1655        NaN    57888.0         20        NaN   72.30000   1804001.28   \n",
      "1656        4.0    59882.0         25        NaN   74.20000    185654.00   \n",
      "\n",
      "      SpentRIGHT  won  statedistrict  \n",
      "0            NaN    0            AL1  \n",
      "1     1240275.64    1            AL2  \n",
      "2        8750.00    0            AL3  \n",
      "3      777837.46    0            AL5  \n",
      "4     1703658.30    1            AL7  \n",
      "5      887310.33    1            AK0  \n",
      "6     1956363.79    0            AZ1  \n",
      "7       21020.76    0            AZ2  \n",
      "8     2645463.16    1            AZ3  \n",
      "9     1092555.25    1            AZ4  \n",
      "10    1721363.62    1            AZ5  \n",
      "11          0.00    0            AZ6  \n",
      "12     684747.70    0            AZ7  \n",
      "13    1653552.57    0            AZ8  \n",
      "14    1172566.21    1            AR1  \n",
      "15    1774143.00    1            AR2  \n",
      "16     641115.26    1            AR3  \n",
      "17    3012726.94    1            AR4  \n",
      "18    1901459.62    1            CA1  \n",
      "19     135634.67    0            CA2  \n",
      "20    1974501.49    1            CA3  \n",
      "21    1703386.70    1            CA4  \n",
      "22      19534.30    0            CA5  \n",
      "23     898626.51    1            CA6  \n",
      "24     108490.78    0            CA7  \n",
      "25    2762400.22    1            CA8  \n",
      "26    1156304.13    1            CA9  \n",
      "27    1714971.13    1           CA10  \n",
      "28    3097236.50    1           CA11  \n",
      "29     772791.25    1           CA12  \n",
      "...          ...  ...            ...  \n",
      "1627   774517.60    1            VA2  \n",
      "1628    71153.64    0            VA3  \n",
      "1629   145572.99    0            VA4  \n",
      "1630   633358.08    1            VA5  \n",
      "1631  1990780.84    1            VA6  \n",
      "1632  1151807.35    1            VA7  \n",
      "1633    80697.11    0            VA8  \n",
      "1634   102048.23    0            VA9  \n",
      "1635  5291181.98    1           VA10  \n",
      "1636    28060.99    0            WA1  \n",
      "1637  1115965.18    1            WA2  \n",
      "1638   112575.05    0            WA3  \n",
      "1639    79051.99    1            WA4  \n",
      "1640   352929.46    0            WA5  \n",
      "1641  1073880.55    1            WA6  \n",
      "1642   468435.42    0            WA7  \n",
      "1643    31422.45    0            WA8  \n",
      "1644  1101068.04    1            WA9  \n",
      "1645        0.00    0           WA10  \n",
      "1646  1067357.71    1            WV1  \n",
      "1647  1324070.16    1            WV2  \n",
      "1648   558823.63    1            WV3  \n",
      "1649    16889.86    0            WI1  \n",
      "1650    38194.40    0            WI2  \n",
      "1651         NaN    0            WI4  \n",
      "1652   296448.98    1            WI5  \n",
      "1653   184143.72    0            WI6  \n",
      "1654   116276.01    0            WI7  \n",
      "1655  1751956.73    0            WI8  \n",
      "1656   183707.78    0            WY0  \n",
      "\n",
      "[1657 rows x 50 columns]\n",
      "      filler  filler.1  filler.2                      filler.3 state  \\\n",
      "1          0         9         8         BRIGHT SR, BOBBY NEAL    AL   \n",
      "5          0         1         2          CRAWFORD, HARRY T JR    AK   \n",
      "6          0        81        86   GOSAR, PAUL ANTHONY ANTHONY    AZ   \n",
      "7          0        93        97                 FRANKS, TRENT    AZ   \n",
      "8          0       104       112                  HULBURD, JON    AZ   \n",
      "9          0       114       118            CONTRERAS, JANET L    AZ   \n",
      "10         0       123       130            MITCHELL, HARRY E.    AZ   \n",
      "13         0       147       151           GIFFORDS, GABRIELLE    AZ   \n",
      "14         0        42        50                  CAUSEY, CHAD    AR   \n",
      "15         0        53        56            ELLIOTT, JOYCE ANN    AR   \n",
      "17         0        73        75             RANKIN, BETH ANNE    AR   \n",
      "18         0       155       158                HANKS, LOREN L    CA   \n",
      "19         0       159       163                 HERGER, WALLY    CA   \n",
      "20         0       163       171                 BERA, AMERISH    CA   \n",
      "24         0       190       192                MILLER, GEORGE    CA   \n",
      "25         0       195       200                  DENNIS, JOHN    CA   \n",
      "27         0       209       220           CLIFT, GARY WILLIAM    CA   \n",
      "36         0       267       268                  DENHAM, JEFF    CA   \n",
      "39         0       284       291                 CAPPS, LOIS G    CA   \n",
      "40         0       293       298        ALLISON, TIMOTHY JAMES    CA   \n",
      "41         0       300       301        CONAWAY, JACQUESE LYNN    CA   \n",
      "42         0       304       306                 DREIER, DAVID    CA   \n",
      "47         0       330       332               BECERRA, XAVIER    CA   \n",
      "52         0       372       377                  FEIN, MATTIE    CA   \n",
      "55         0       390       393           ANDRE, LARRY STEVEN    CA   \n",
      "56         0       393       394             AVALOS, CHRISTINA    CA   \n",
      "57         0       395       397                  LEWIS, JERRY    CA   \n",
      "60         0       406       407                  CALVERT, KEN    CA   \n",
      "62         0       413       417               ARNOLD, KENNETH    CA   \n",
      "68         0       443       446             HUNTER, DUNCAN D.    CA   \n",
      "...      ...       ...       ...                           ...   ...   \n",
      "1558       0      9587      9591         BALCHUNIS, MARY ELLEN    PA   \n",
      "1559       0      9592      9598            FITZPATRICK, BRIAN    PA   \n",
      "1562       0      9604      9606                 BARLETTA, LOU    PA   \n",
      "1563       0      9607      9608              MCCLELLAND, ERIN    PA   \n",
      "1566       0      9620      9625  HARTMAN, CHRISTINA MARIE MS.    PA   \n",
      "1568       0      9640      9645            CICILLINE, DAVID N    RI   \n",
      "1571       0      9657      9658                   BJORN, ARIK    SC   \n",
      "1576       0      9677      9678                    HYMAN, MAL    SC   \n",
      "1577       0      9679      9680                  HAWKS, PAULA    SD   \n",
      "1607       0      9876      9881                 GALLEGO, PETE    TX   \n",
      "1609       0      9889      9891              THOMAS, KATHLEEN    TX   \n",
      "1611       0      9898      9902           BARRERA, RAUL (ROY)    TX   \n",
      "1617       0      9936      9940          MITCHELL, MONTE MARK    TX   \n",
      "1618       0      9941      9943             GONZALEZ, REY DR.    TX   \n",
      "1619       0      9945      9948                DOGGETT, LLOYD    TX   \n",
      "1620       0      9949      9953                  BABIN, BRIAN    TX   \n",
      "1622       0      9958      9960   ALBARRAN, CHARLENE MCARTHUR    UT   \n",
      "1624       0      9965      9968                     LOVE, MIA    UT   \n",
      "1625       0     10017     10018                CLAWSON, ERICA    VT   \n",
      "1627       0      9971      9972           BROWN, SHAUN DENISE    VA   \n",
      "1630       0      9985      9987        DITTMAR, JANE DESIMONE    VA   \n",
      "1634       0     10004     10006            GRIFFITH, H MORGAN    VA   \n",
      "1638       0     10034     10039        HERRERA BEUTLER, JAIME    WA   \n",
      "1643       0     10072     10077                REICHERT, DAVE    WA   \n",
      "1645       0     10084     10086                  HECK, DENNIS    WA   \n",
      "1646       0     10125     10126               MANYPENNY, MIKE    WV   \n",
      "1648       0     10134     10135           DETCH, MATTHEW PAUL    WV   \n",
      "1654       0     10111     10118                   DUFFY, SEAN    WI   \n",
      "1655       0     10118     10124       GALLAGHER, MICHAEL JOHN    WI   \n",
      "1656       0     10139     10153        CHENEY, ELIZABETH MRS.    WY   \n",
      "\n",
      "      district candpartyLEFT  filler.4  filler.5  filler.6      ...        \\\n",
      "1            2           DEM      2010  0.407024      19.3      ...         \n",
      "5            0           DEM      2010       NaN      27.0      ...         \n",
      "6            1           REP      2010  0.441068      19.2      ...         \n",
      "7            2           REP      2010  0.439815      21.9      ...         \n",
      "8            3           DEM      2010  0.445441      33.9      ...         \n",
      "9            4           REP      2010  0.445441      13.6      ...         \n",
      "10           5           DEM      2010  0.445441      43.6      ...         \n",
      "13           8           DEM      2010  0.502409      34.1      ...         \n",
      "14           1           DEM      2010  0.386129      13.7      ...         \n",
      "15           2           DEM      2010  0.447858      25.8      ...         \n",
      "17           4           REP      2010  0.404828      15.0      ...         \n",
      "18           1           REP      2010  0.704198      28.4      ...         \n",
      "19           2           REP      2010  0.491595      20.4      ...         \n",
      "20           3           DEM      2010  0.595003      29.0      ...         \n",
      "24           7           DEM      2010  0.681005      25.5      ...         \n",
      "25           8           REP      2010  0.861222      50.1      ...         \n",
      "27          10           REP      2010  0.695085      39.4      ...         \n",
      "36          19           REP      2010  0.489362      21.6      ...         \n",
      "39          23           DEM      2010  0.568008      27.8      ...         \n",
      "40          24           DEM      2010  0.581761      34.3      ...         \n",
      "41          25           DEM      2010  0.675910      21.2      ...         \n",
      "42          26           REP      2010  0.676561      36.7      ...         \n",
      "47          31           DEM      2010  0.701108      20.0      ...         \n",
      "52          36           REP      2010  0.701108      43.2      ...         \n",
      "55          39           REP      2010  0.701108      16.9      ...         \n",
      "56          40           DEM      2010  0.480547      31.2      ...         \n",
      "57          41           REP      2010  0.521614      20.0      ...         \n",
      "60          44           REP      2010  0.491709      25.8      ...         \n",
      "62          46           DEM      2010  0.646809      40.5      ...         \n",
      "68          52           REP      2010  0.547835      32.3      ...         \n",
      "...        ...           ...       ...       ...       ...      ...         \n",
      "1558         7           DEM      2016  0.563603      42.2      ...         \n",
      "1559         8           REP      2016  0.678581      38.3      ...         \n",
      "1562        11           REP      2016  0.528180      23.5      ...         \n",
      "1563        12           DEM      2016  0.494574      32.1      ...         \n",
      "1566        16           DEM      2016  0.464458      25.4      ...         \n",
      "1568         1           DEM      2016  0.660627      32.2      ...         \n",
      "1571         2           DEM      2016       NaN      32.9      ...         \n",
      "1576         7           DEM      2016       NaN      20.4      ...         \n",
      "1577         0           DEM      2016  0.407809      27.5      ...         \n",
      "1607        23           DEM      2016  0.553952      21.4      ...         \n",
      "1609        25           DEM      2016  0.563523      35.8      ...         \n",
      "1611        27           DEM      2016  0.548980      18.7      ...         \n",
      "1617        33           REP      2016       NaN       9.4      ...         \n",
      "1618        34           REP      2016       NaN      15.1      ...         \n",
      "1619        35           DEM      2016       NaN      19.0      ...         \n",
      "1620        36           REP      2016       NaN      18.8      ...         \n",
      "1622         2           DEM      2016       NaN      31.4      ...         \n",
      "1624         4           REP      2016       NaN      28.2      ...         \n",
      "1625         0           OTH      2016  0.682490      36.2      ...         \n",
      "1627         2           DEM      2016  0.585814      33.2      ...         \n",
      "1630         5           DEM      2016  0.457956      26.5      ...         \n",
      "1634         9           REP      2016  0.354572      19.7      ...         \n",
      "1638         3           REP      2016  0.524482      23.8      ...         \n",
      "1643         8           REP      2016  0.663278      33.0      ...         \n",
      "1645        10           DEM      2016       NaN      26.7      ...         \n",
      "1646         1           DEM      2016  0.363358      21.8      ...         \n",
      "1648         3           DEM      2016  0.332106      15.6      ...         \n",
      "1654         7           REP      2016  0.505058      22.0      ...         \n",
      "1655         8           REP      2016  0.480119      24.9      ...         \n",
      "1656         0           REP      2016       NaN      26.0      ...         \n",
      "\n",
      "     filler.29  filler.30  filler.31  filler.32  filler.33  filler.34  \\\n",
      "1        False        0.0    40567.0        161      48.55   80.00000   \n",
      "5        False        4.0    64576.0         42      70.35   78.10000   \n",
      "6         True        0.0    40854.0        100        100   77.15333   \n",
      "7         True        4.0    47963.0          6        100   77.50000   \n",
      "8        False        1.0    52327.0         44      22.05   76.50000   \n",
      "9         True        0.0    31498.0         40        100   77.19474   \n",
      "10       False        0.0    55876.0          4      37.23   77.49048   \n",
      "13       False        NaN    49825.0         66      48.31   77.70000   \n",
      "14       False        4.0    33862.0         14      71.79   77.20000   \n",
      "15       False        0.0    43685.0          6      61.69   77.00000   \n",
      "17        True        NaN    35312.0          1        100   79.30000   \n",
      "18        True        1.0    48003.0          1        100   78.88947   \n",
      "19        True        0.0    42249.0          5        100   78.17143   \n",
      "20       False        0.0    61853.0         12        100   77.40000   \n",
      "24       False        4.0    61315.0         16      57.07   77.50000   \n",
      "25        True        4.0    69926.0         78        100   76.60000   \n",
      "27        True        4.0    75457.0          6        100   76.90000   \n",
      "36        True        4.0    51553.0        500      52.56   78.00000   \n",
      "39       False        4.0    54049.0         20      36.08   75.92632   \n",
      "40       False        4.0    74103.0        500        100   75.37143   \n",
      "41       False        4.0    57862.0        188        100   74.90000   \n",
      "42        True        4.0    75086.0        500        100   75.20000   \n",
      "47       False        0.0    35194.0         33      56.68   72.41333   \n",
      "52        True        0.0    65717.0        500      59.09   74.90000   \n",
      "55        True        4.0    53439.0         68      29.25   74.60000   \n",
      "56       False        4.0    65243.0         40        100   73.40000   \n",
      "57        True        4.0    49506.0         37        100   71.10000   \n",
      "60        True        4.0    67381.0          9        100   74.00000   \n",
      "62       False        4.0    71332.0         57        100   75.72857   \n",
      "68        True        0.0    67671.0         61      71.26   77.80000   \n",
      "...        ...        ...        ...        ...        ...        ...   \n",
      "1558     False        4.0    85881.0         37      99.88   74.90000   \n",
      "1559      True        0.0    80199.0         19        NaN   74.48000   \n",
      "1562      True        4.0    53833.0          1        NaN   75.25789   \n",
      "1563     False        0.0    60387.0         45      99.25   72.58095   \n",
      "1566     False        0.0    57144.0        500        NaN   70.80000   \n",
      "1568     False        0.0    56547.0        500        NaN   77.60000   \n",
      "1571     False        4.0    57253.0        107      81.61   65.50000   \n",
      "1576     False        NaN    43146.0         20        100   69.99048   \n",
      "1577     False        0.0    54467.0        108        100   72.60000   \n",
      "1607     False        0.0    51293.0          9      40.96   70.00000   \n",
      "1609     False        0.0    69083.0         92        100   70.98571   \n",
      "1611     False        0.0    53047.0        500        100   71.70000   \n",
      "1617      True        4.0    38445.0         89      73.48   76.10000   \n",
      "1618      True        0.0    37989.0        500        100   76.10000   \n",
      "1619     False        0.0    48490.0         27        100   76.10000   \n",
      "1620     False        NaN    58984.0        500        NaN   76.10000   \n",
      "1622     False        0.0    60700.0        109        NaN   71.30526   \n",
      "1624      True        NaN    68711.0        500        NaN   73.20000   \n",
      "1625      True        4.0    57677.0         55      98.85   68.50000   \n",
      "1627     False        0.0    66953.0         17        NaN   67.30000   \n",
      "1630     False        0.0    52237.0         20        NaN   71.50000   \n",
      "1634      True        0.0    41698.0         73        NaN   72.30000   \n",
      "1638      True        0.0    61304.0          5        NaN   73.32000   \n",
      "1643      True        0.0    79516.0         80        NaN   68.50000   \n",
      "1645     False        NaN    62206.0          5        NaN   73.10000   \n",
      "1646     False        0.0    45611.0          4        100   68.30000   \n",
      "1648     False        NaN    37728.0        500        100   69.60000   \n",
      "1654      True        0.0    51738.0         15        NaN   71.90000   \n",
      "1655      True        NaN    57888.0         20        NaN   72.30000   \n",
      "1656      True        4.0    59882.0         25        NaN   74.20000   \n",
      "\n",
      "      RaisedRIGHT  SpentRIGHT  won  statedistrict  \n",
      "1      1253557.11  1240275.64    1            AL2  \n",
      "5      1001015.37   887310.33    1            AK0  \n",
      "6      1957921.74  1956363.79    0            AZ1  \n",
      "7        21020.76    21020.76    0            AZ2  \n",
      "8      2653070.20  2645463.16    1            AZ3  \n",
      "9      1014290.67  1092555.25    1            AZ4  \n",
      "10     1732731.48  1721363.62    1            AZ5  \n",
      "13     1692503.57  1653552.57    0            AZ8  \n",
      "14     1263738.21  1172566.21    1            AR1  \n",
      "15     1855578.10  1774143.00    1            AR2  \n",
      "17     2426280.46  3012726.94    1            AR4  \n",
      "18     1912475.17  1901459.62    1            CA1  \n",
      "19      137553.75   135634.67    0            CA2  \n",
      "20     2025541.46  1974501.49    1            CA3  \n",
      "24      108490.78   108490.78    0            CA7  \n",
      "25     2597318.64  2762400.22    1            CA8  \n",
      "27     1764504.06  1714971.13    1           CA10  \n",
      "36       44517.00    46862.00    0           CA19  \n",
      "39      611864.23   607298.83    0           CA23  \n",
      "40      783256.55   811193.84    1           CA24  \n",
      "41     1254589.69  1334109.46    1           CA25  \n",
      "42      305157.48   207192.10    0           CA26  \n",
      "47       14761.00    13851.72    0           CA31  \n",
      "52     1203707.21  1301447.56    1           CA36  \n",
      "55      723863.07   741142.01    1           CA39  \n",
      "56     1797849.31  1661978.10    1           CA40  \n",
      "57       42720.27    42859.78    0           CA41  \n",
      "60      546700.86   561785.82    0           CA44  \n",
      "62      404285.00   382318.61    1           CA46  \n",
      "68       35815.94    35815.94    0           CA52  \n",
      "...           ...         ...  ...            ...  \n",
      "1558   2419635.87  2155482.68    1            PA7  \n",
      "1559   2792763.35  2778029.33    0            PA8  \n",
      "1562    132488.98   123748.07    0           PA11  \n",
      "1563   1734459.97  1651927.35    1           PA12  \n",
      "1566   1487254.00  1453232.45    1           PA16  \n",
      "1568    103111.34    87316.24    0            RI1  \n",
      "1571   1151886.85   743749.03    1            SC2  \n",
      "1576   1082645.07   935571.49    1            SC7  \n",
      "1577   2328820.31  3054652.98    1            SD0  \n",
      "1607   4061684.94  4109803.72    1           TX23  \n",
      "1609   1692142.67  1189560.93    1           TX25  \n",
      "1611   1099142.68  1108699.54    1           TX27  \n",
      "1617   1402920.35  1238209.49    1           TX33  \n",
      "1618   1215318.70  1044105.58    1           TX34  \n",
      "1619    132376.47   132119.24    0           TX35  \n",
      "1620          NaN         NaN    0           TX36  \n",
      "1622    911494.54   949475.02    1            UT2  \n",
      "1624   2096381.92  2083377.65    0            UT4  \n",
      "1625    980546.10   740624.93    1            VT0  \n",
      "1627    839484.99   774517.60    1            VA2  \n",
      "1630    647091.95   633358.08    1            VA5  \n",
      "1634    102048.52   102048.23    0            VA9  \n",
      "1638    112575.05   112575.05    0            WA3  \n",
      "1643     31422.45    31422.45    0            WA8  \n",
      "1645         0.00        0.00    0           WA10  \n",
      "1646   1147264.04  1067357.71    1            WV1  \n",
      "1648   1269625.19   558823.63    1            WV3  \n",
      "1654    116276.39   116276.01    0            WI7  \n",
      "1655   1804001.28  1751956.73    0            WI8  \n",
      "1656    185654.00   183707.78    0            WY0  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"partiesthushcrossy.csv\", sep = \"#\", engine = \"python\")\n",
    "\n",
    "what[\"statedistrict\"] = what[\"state\"] + what[\"district\"].map(str)\n",
    "\n",
    "# print(what)\n",
    "print(what)\n",
    "uncontestedraces = what.loc[what['statedistrict'].isin(combinelist)]\n",
    "\n",
    "# print(uncontestedraces)\n",
    "\n",
    "contestedraces = pandas.concat([what, uncontestedraces, uncontestedraces]).drop_duplicates(keep=False)\n",
    "\n",
    "print(contestedraces)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "contestedraces.to_csv(\"contestedraces.csv\", sep='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.04965141 0.14031084 0.10647293 0.05542664 0.09308397 0.26207549\n",
      " 0.15892597 0.13405273]\n",
      "[0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
      " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1\n",
      " 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0\n",
      " 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0\n",
      " 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
      " 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
      " 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
      " 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0\n",
      " 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1\n",
      " 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 0\n",
      " 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1\n",
      " 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1\n",
      " 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0\n",
      " 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0\n",
      " 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0]\n",
      "Below is our training data analysis\n",
      "235\n",
      "187\n",
      "0.7957446808510639 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "70\n",
      "59\n",
      "0.8428571428571429 is our testing accuracy\n",
      "look down here for oob\n",
      "0.826\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=2,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "# print(train)\n",
    "# trainingx = train[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# trainingy = train[['won']]\n",
    "\n",
    "# print(tester)\n",
    "\n",
    "# testingx = tester[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# testingy = tester[['won']]\n",
    "\n",
    "\n",
    "# trainingx = d1[d1.index % 4 != 0]\n",
    "# testingx = d1[d1.index % 4 == 0] \n",
    "\n",
    "# trainingy = d2[d2.index % 4 != 0]\n",
    "# testingy = d2[d2.index % 4 == 0] \n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.05336444 0.09457244 0.11004038 0.08377849 0.04710156 0.25741777\n",
      " 0.18207279 0.17165212]\n",
      "[1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0\n",
      " 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0\n",
      " 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0\n",
      " 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
      " 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1\n",
      " 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
      " 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
      " 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0\n",
      " 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0]\n",
      "[1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
      " 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
      " 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0\n",
      " 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1\n",
      " 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 0\n",
      " 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
      " 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
      " 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
      " 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0\n",
      " 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0\n",
      " 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0]\n",
      "Below is our training data analysis\n",
      "234\n",
      "214\n",
      "0.9145299145299145 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "71\n",
      "67\n",
      "0.9436619718309859 is our testing accuracy\n",
      "look down here for oob\n",
      "0.846\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=4,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.05720429 0.09656602 0.14559927 0.11508101 0.07582116 0.18072673\n",
      " 0.17652594 0.15247556]\n",
      "[0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1\n",
      " 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1\n",
      " 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
      " 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1\n",
      " 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0\n",
      " 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1\n",
      " 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1\n",
      " 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1]\n",
      "[0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1\n",
      " 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1\n",
      " 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
      " 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1\n",
      " 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0\n",
      " 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
      " 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1\n",
      " 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1]\n",
      "Below is our training data analysis\n",
      "237\n",
      "226\n",
      "0.9535864978902954 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "68\n",
      "52\n",
      "0.7647058823529411 is our testing accuracy\n",
      "look down here for oob\n",
      "0.842\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=6,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "# print(train)\n",
    "# trainingx = train[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# trainingy = train[['won']]\n",
    "\n",
    "# print(tester)\n",
    "\n",
    "# testingx = tester[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# testingy = tester[['won']]\n",
    "\n",
    "\n",
    "# trainingx = d1[d1.index % 4 != 0]\n",
    "# testingx = d1[d1.index % 4 == 0] \n",
    "\n",
    "# trainingy = d2[d2.index % 4 != 0]\n",
    "# testingy = d2[d2.index % 4 == 0] \n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.07551047 0.09395647 0.13388219 0.1118523  0.06340046 0.167435\n",
      " 0.18593688 0.16802622]\n",
      "[1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0\n",
      " 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0\n",
      " 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0\n",
      " 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1\n",
      " 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
      " 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0\n",
      " 1 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1\n",
      " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0]\n",
      "[1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0\n",
      " 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0\n",
      " 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0\n",
      " 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1\n",
      " 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
      " 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0\n",
      " 1 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1\n",
      " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0]\n",
      "Below is our training data analysis\n",
      "236\n",
      "233\n",
      "0.9872881355932204 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "69\n",
      "58\n",
      "0.8405797101449275 is our testing accuracy\n",
      "look down here for oob\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=8,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "# print(train)\n",
    "# trainingx = train[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# trainingy = train[['won']]\n",
    "\n",
    "# print(tester)\n",
    "\n",
    "# testingx = tester[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "#             'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "# testingy = tester[['won']]\n",
    "\n",
    "\n",
    "# trainingx = d1[d1.index % 4 != 0]\n",
    "# testingx = d1[d1.index % 4 == 0] \n",
    "\n",
    "# trainingy = d2[d2.index % 4 != 0]\n",
    "# testingy = d2[d2.index % 4 == 0] \n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.09797347 0.0932106  0.15932323 0.12061211 0.07500699 0.13255902\n",
      " 0.15731012 0.16400446]\n",
      "[0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1\n",
      " 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
      " 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
      " 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0\n",
      " 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1\n",
      " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0\n",
      " 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0]\n",
      "[0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1\n",
      " 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
      " 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
      " 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0\n",
      " 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1\n",
      " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0\n",
      " 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0]\n",
      "Below is our training data analysis\n",
      "228\n",
      "228\n",
      "1.0 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "77\n",
      "61\n",
      "0.7922077922077922 is our testing accuracy\n",
      "look down here for oob\n",
      "0.824\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=10,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.10333117 0.07323129 0.12961887 0.11917764 0.0823691  0.140561\n",
      " 0.18624209 0.16546885]\n",
      "[0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1\n",
      " 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1\n",
      " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1\n",
      " 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
      " 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1\n",
      " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1\n",
      " 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
      " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1\n",
      " 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1\n",
      " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1\n",
      " 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
      " 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1\n",
      " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1\n",
      " 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
      " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0]\n",
      "Below is our training data analysis\n",
      "237\n",
      "237\n",
      "1.0 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "68\n",
      "61\n",
      "0.8970588235294118 is our testing accuracy\n",
      "look down here for oob\n",
      "0.832\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=12,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.09051679 0.09202507 0.13142567 0.1116032  0.0934387  0.16699952\n",
      " 0.16395048 0.15004058]\n",
      "[1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1\n",
      " 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n",
      " 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
      " 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1]\n",
      "[1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1\n",
      " 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n",
      " 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
      " 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1]\n",
      "Below is our training data analysis\n",
      "240\n",
      "240\n",
      "1.0 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "65\n",
      "52\n",
      "0.8 is our testing accuracy\n",
      "look down here for oob\n",
      "0.876\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=14,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.084358   0.09359105 0.15174257 0.13468976 0.08074272 0.13714342\n",
      " 0.15873575 0.15899672]\n",
      "[1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1\n",
      " 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
      " 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
      " 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
      " 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1\n",
      " 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1]\n",
      "[1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1\n",
      " 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
      " 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
      " 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
      " 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1\n",
      " 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1]\n",
      "Below is our training data analysis\n",
      "240\n",
      "240\n",
      "1.0 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "65\n",
      "48\n",
      "0.7384615384615385 is our testing accuracy\n",
      "look down here for oob\n",
      "0.842\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=16,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.08974383 0.07969022 0.13503816 0.12978891 0.08390592 0.15169903\n",
      " 0.17562979 0.15450415]\n",
      "[0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1\n",
      " 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0\n",
      " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0\n",
      " 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1\n",
      " 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0\n",
      " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0\n",
      " 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Below is our training data analysis\n",
      "233\n",
      "233\n",
      "1.0 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "72\n",
      "61\n",
      "0.8472222222222222 is our testing accuracy\n",
      "look down here for oob\n",
      "0.816\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=18,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "[0.10520985 0.07632001 0.14515776 0.11798121 0.09077332 0.14270816\n",
      " 0.17113464 0.15071505]\n",
      "[0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
      " 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
      " 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1\n",
      " 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
      " 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0]\n",
      "[0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
      " 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
      " 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1\n",
      " 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
      " 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0]\n",
      "Below is our training data analysis\n",
      "238\n",
      "238\n",
      "1.0 is our training accuracy\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Below is our testing data analysis\n",
      "67\n",
      "58\n",
      "0.8656716417910447 is our testing accuracy\n",
      "look down here for oob\n",
      "0.822\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"contestedraces.csv\", sep='#')\n",
    "\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "# print(d2)\n",
    "# print(d2)\n",
    "# d2 = np.ravel(d2)\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=4,\n",
    "#                             n_informative=2, n_redundant=0,\n",
    "#                             random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "# X, y = d1, d2\n",
    "# print(y)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=20,\n",
    "                              random_state=0, oob_score = True)\n",
    "\n",
    "# X = d1.sample(2718)\n",
    "\n",
    "# trainingx = pandas.concat([d1, testingx, testingx]).drop_duplicates(keep=False)\n",
    "\n",
    "# print(trainingx)\n",
    "# # print(testingy)\n",
    "# trainingy = pandas.concat([d2, testingy, testingy]).drop_duplicates(keep=False)\n",
    "\n",
    "# # print(trainingx)\n",
    "# print(trainingy)\n",
    "\n",
    "# print(d2)\n",
    "toterows = np.ravel(d2)\n",
    "print(sum(toterows))\n",
    "\n",
    "combine = d1.join(d2, lsuffix='SpentRIGHT', rsuffix='won')\n",
    "\n",
    "# print(combine)\n",
    "\n",
    "combine = combine.sample(frac = 1)\n",
    "\n",
    "d1 = combine[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "d2 = combine[['won']]\n",
    "\n",
    "\n",
    "trainingx = d1.iloc[:500]\n",
    "\n",
    "testingx = d1.iloc[500:]\n",
    "\n",
    "# print(testingx)\n",
    "\n",
    "trainingy = d2.iloc[:500]\n",
    "testingy = d2.iloc[500:]\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(trainingx, np.ravel(trainingy))\n",
    "# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth = 4, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=True, random_state=0, verbose=0, warm_start=True)\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "# print(testingx)\n",
    "traininganswers = clf.predict(trainingx)\n",
    "testinganswers = clf.predict(testingx)\n",
    "# print(testinganswers)\n",
    "\n",
    "print(np.ravel(trainingy))\n",
    "print(traininganswers)\n",
    "\n",
    "realanswers = np.ravel(trainingy)\n",
    "\n",
    "\n",
    "print(\"Below is our training data analysis\")\n",
    "summer = sum(realanswers)\n",
    "print(summer)\n",
    "\n",
    "numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "print(\"Below is our testing data analysis\")\n",
    "realtestanswers = np.ravel(testingy)\n",
    "\n",
    "summer = sum(realtestanswers)\n",
    "print(summer)\n",
    "numcorrect = np.dot(realtestanswers, testinganswers)\n",
    "\n",
    "print(numcorrect)\n",
    "print(str(numcorrect/summer) + \" is our testing accuracy\")\n",
    "\n",
    "print(\"look down here for oob\")\n",
    "print(clf.oob_score_)\n",
    "\n",
    "oobscores.append(( clf.n_estimators, clf.max_depth,clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(200, 2, 0.826), (200, 4, 0.846), (200, 6, 0.842), (200, 8, 0.85), (200, 10, 0.824), (200, 12, 0.832), (200, 14, 0.876), (200, 16, 0.842), (200, 18, 0.816), (200, 20, 0.822)]\n"
     ]
    }
   ],
   "source": [
    "print(oobscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "oobdepth = []\n",
    "deepscore = []\n",
    "for x,y,z in oobscores:\n",
    "    oobdepth.append(y)\n",
    "    deepscore.append(z)\n",
    "    \n",
    "plt.plot(oobdepth, deepscore, 'b-')\n",
    "plt.plot(oobdepth, deepscore, 'bo')\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"oob (CV) score\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.985, 0.015], [0.07, 0.93], [0.0, 1.0], [0.98, 0.02], [0.99, 0.01], [0.005, 0.995], [0.005, 0.995], [0.97, 0.03], [0.71, 0.29], [0.985, 0.015], [0.14, 0.86], [1.0, 0.0], [0.015, 0.985], [0.32, 0.68], [0.425, 0.575], [1.0, 0.0], [0.49, 0.51], [0.0, 1.0], [0.005, 0.995], [0.29, 0.71], [1.0, 0.0], [1.0, 0.0], [0.435, 0.565], [0.0, 1.0], [0.755, 0.245], [0.955, 0.045], [0.96, 0.04], [0.0, 1.0], [0.5, 0.5], [0.99, 0.01], [0.935, 0.065], [0.995, 0.005], [0.005, 0.995], [0.015, 0.985], [0.775, 0.225], [0.0, 1.0], [1.0, 0.0], [0.005, 0.995], [0.0, 1.0], [0.395, 0.605], [0.55, 0.45], [0.04, 0.96], [0.82, 0.18], [0.29, 0.71], [0.99, 0.01], [0.96, 0.04], [0.06, 0.94], [0.205, 0.795], [0.0, 1.0], [0.98, 0.02], [0.925, 0.075], [1.0, 0.0], [0.97, 0.03], [0.985, 0.015], [0.24, 0.76], [0.95, 0.05], [0.12, 0.88], [0.48, 0.52], [0.02, 0.98], [0.0, 1.0], [1.0, 0.0], [0.215, 0.785], [0.515, 0.485], [0.305, 0.695], [0.975, 0.025], [0.005, 0.995], [0.285, 0.715], [0.235, 0.765], [0.29, 0.71], [0.01, 0.99], [1.0, 0.0], [0.99, 0.01], [1.0, 0.0], [0.995, 0.005], [0.37, 0.63], [0.8, 0.2], [1.0, 0.0], [0.46, 0.54], [0.0, 1.0], [0.995, 0.005], [1.0, 0.0], [0.0, 1.0], [0.21, 0.79], [1.0, 0.0], [0.995, 0.005], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.01, 0.99], [0.655, 0.345], [0.305, 0.695], [0.0, 1.0], [0.005, 0.995], [0.005, 0.995], [0.96, 0.04], [0.035, 0.965], [0.685, 0.315], [0.195, 0.805], [0.17, 0.83], [0.5, 0.5], [0.045, 0.955], [1.0, 0.0], [0.0, 1.0], [0.49, 0.51], [0.575, 0.425], [0.38, 0.62], [0.98, 0.02], [1.0, 0.0], [0.0, 1.0], [0.995, 0.005], [0.685, 0.315], [0.445, 0.555], [0.97, 0.03], [0.98, 0.02], [0.01, 0.99], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.955, 0.045], [0.005, 0.995], [0.985, 0.015], [1.0, 0.0], [0.22, 0.78], [1.0, 0.0], [0.02, 0.98], [0.955, 0.045], [0.0, 1.0], [0.365, 0.635], [0.065, 0.935], [0.055, 0.945], [0.0, 1.0], [0.075, 0.925], [1.0, 0.0], [0.55, 0.45], [0.31, 0.69], [0.45, 0.55], [0.0, 1.0], [0.0, 1.0], [0.97, 0.03], [0.98, 0.02], [0.005, 0.995], [0.625, 0.375], [0.89, 0.11], [0.965, 0.035], [1.0, 0.0], [0.07, 0.93], [1.0, 0.0], [1.0, 0.0], [0.515, 0.485], [0.545, 0.455], [0.95, 0.05], [0.305, 0.695], [0.76, 0.24], [0.005, 0.995], [0.685, 0.315], [0.31, 0.69], [0.735, 0.265], [1.0, 0.0], [0.98, 0.02], [0.24, 0.76], [0.225, 0.775], [0.985, 0.015], [0.28, 0.72], [0.035, 0.965], [0.98, 0.02], [0.485, 0.515], [1.0, 0.0], [0.975, 0.025], [0.0, 1.0], [0.01, 0.99], [0.31, 0.69], [0.425, 0.575], [0.005, 0.995], [0.0, 1.0], [0.015, 0.985], [0.05, 0.95], [0.27, 0.73], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.345, 0.655], [0.985, 0.015], [0.84, 0.16], [0.9, 0.1], [0.8, 0.2], [0.23, 0.77], [0.01, 0.99], [0.0, 1.0], [1.0, 0.0], [0.385, 0.615], [0.425, 0.575], [0.355, 0.645], [1.0, 0.0], [0.05, 0.95], [0.305, 0.695], [1.0, 0.0], [0.74, 0.26], [0.24, 0.76], [0.34, 0.66], [0.925, 0.075], [0.99, 0.01], [0.89, 0.11], [0.995, 0.005], [0.005, 0.995], [0.235, 0.765], [1.0, 0.0], [0.0, 1.0], [0.205, 0.795], [0.005, 0.995], [0.975, 0.025], [0.38, 0.62], [0.0, 1.0], [0.995, 0.005], [1.0, 0.0], [0.995, 0.005], [1.0, 0.0], [0.005, 0.995], [0.645, 0.355], [1.0, 0.0], [0.57, 0.43], [0.975, 0.025], [0.02, 0.98], [1.0, 0.0], [0.62, 0.38], [0.365, 0.635], [0.55, 0.45], [0.925, 0.075], [0.0, 1.0], [0.195, 0.805], [0.255, 0.745], [0.725, 0.275], [0.95, 0.05], [0.995, 0.005], [0.355, 0.645], [0.085, 0.915], [0.02, 0.98], [0.525, 0.475], [0.995, 0.005], [0.12, 0.88], [0.29, 0.71], [0.885, 0.115], [0.225, 0.775], [0.47, 0.53], [0.005, 0.995], [0.0, 1.0], [0.0, 1.0], [0.585, 0.415], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.185, 0.815], [0.04, 0.96], [0.01, 0.99], [0.605, 0.395], [0.36, 0.64], [0.0, 1.0], [0.945, 0.055], [0.225, 0.775], [0.995, 0.005], [0.385, 0.615], [0.26, 0.74], [0.37, 0.63], [0.36, 0.64], [0.365, 0.635], [1.0, 0.0], [0.635, 0.365], [0.005, 0.995], [0.23, 0.77], [0.0, 1.0], [0.005, 0.995], [1.0, 0.0], [0.205, 0.795], [0.955, 0.045], [0.285, 0.715], [0.045, 0.955], [1.0, 0.0], [0.98, 0.02], [0.49, 0.51], [0.78, 0.22], [0.37, 0.63], [0.055, 0.945], [1.0, 0.0], [0.07, 0.93], [0.0, 1.0], [0.995, 0.005], [0.445, 0.555], [1.0, 0.0], [1.0, 0.0], [0.185, 0.815], [1.0, 0.0], [0.26, 0.74], [0.0, 1.0], [0.515, 0.485], [0.555, 0.445], [0.98, 0.02], [0.12, 0.88], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.33, 0.67], [1.0, 0.0], [0.385, 0.615], [0.8, 0.2], [1.0, 0.0], [0.0, 1.0], [0.565, 0.435], [1.0, 0.0], [0.935, 0.065], [0.385, 0.615], [0.27, 0.73], [0.875, 0.125], [0.93, 0.07], [0.65, 0.35], [0.84, 0.16], [0.825, 0.175], [0.385, 0.615], [0.005, 0.995], [0.805, 0.195], [0.33, 0.67], [0.475, 0.525], [0.39, 0.61], [0.61, 0.39], [1.0, 0.0], [0.01, 0.99], [0.575, 0.425], [0.005, 0.995], [0.99, 0.01], [0.67, 0.33], [0.315, 0.685], [0.99, 0.01], [1.0, 0.0], [0.54, 0.46], [0.05, 0.95], [0.99, 0.01], [0.975, 0.025], [0.74, 0.26], [0.045, 0.955], [0.075, 0.925], [0.945, 0.055], [0.995, 0.005], [0.155, 0.845], [1.0, 0.0], [0.345, 0.655], [0.085, 0.915], [0.005, 0.995], [0.42, 0.58], [0.37, 0.63], [0.895, 0.105], [0.645, 0.355], [1.0, 0.0], [0.97, 0.03], [0.0, 1.0], [0.0, 1.0], [0.005, 0.995], [0.975, 0.025], [0.91, 0.09], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.68, 0.32], [0.205, 0.795], [0.52, 0.48], [0.995, 0.005], [0.085, 0.915], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.215, 0.785], [0.945, 0.055], [0.38, 0.62], [0.23, 0.77], [0.005, 0.995], [0.0, 1.0], [0.995, 0.005], [0.835, 0.165], [1.0, 0.0], [0.005, 0.995], [1.0, 0.0], [0.575, 0.425], [0.0, 1.0], [0.0, 1.0], [0.445, 0.555], [0.0, 1.0], [0.655, 0.345], [0.945, 0.055], [0.38, 0.62], [1.0, 0.0], [0.19, 0.81], [0.515, 0.485], [0.88, 0.12], [0.02, 0.98], [1.0, 0.0], [0.44, 0.56], [0.02, 0.98], [0.32, 0.68], [0.015, 0.985], [0.98, 0.02], [0.4, 0.6], [0.925, 0.075], [0.0, 1.0], [0.0, 1.0], [0.96, 0.04], [0.475, 0.525], [0.535, 0.465], [0.95, 0.05], [0.995, 0.005], [0.06, 0.94], [0.435, 0.565], [0.935, 0.065], [0.995, 0.005], [1.0, 0.0]]\n",
      "look up\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " ???? \n",
      " \n",
      " \n",
      " ???? FAIL \n",
      " \n",
      " \n",
      "\n",
      "['AZ', 5, 'UNK', 1.0]\n",
      "[['AL', 1, 'R', 0.985], ['AL', 2, 'R', 0.93], ['AL', 3, 'R', 1.0], ['AL', 4, 'R', 0.98], ['AL', 5, 'R', 0.99], ['AL', 6, 'R', 0.995], ['AK', 0, 'R', 0.995], ['AZ', 1, 'D', 0.97], ['AZ', 2, 'D', 0.71], ['AZ', 3, 'D', 0.985], ['AZ', 4, 'R', 0.86], ['AZ', 5, 'UNK', 1.0], ['AZ', 6, 'R', 0.985], ['AZ', 8, 'D', 0.68], ['AZ', 9, 'D', 0.575], ['AR', 1, 'R', 1.0], ['AR', 2, 'D', 0.51], ['AR', 3, 'R', 1.0], ['AR', 4, 'R', 0.995], ['CA', 1, 'R', 0.71], ['CA', 2, 'D', 1.0], ['CA', 3, 'D', 1.0], ['CA', 4, 'D', 0.565], ['CA', 5, 'D', 1.0], ['CA', 6, 'D', 0.755], ['CA', 7, 'D', 0.955], ['CA', 8, 'R', 0.96], ['CA', 9, 'D', 1.0], ['CA', 10, 'R', 0.5], ['CA', 11, 'D', 0.99], ['CA', 12, 'D', 0.935], ['CA', 13, 'D', 0.995], ['CA', 14, 'D', 0.995], ['CA', 15, 'D', 0.985], ['CA', 16, 'D', 0.775], ['CA', 17, 'D', 1.0], ['CA', 18, 'D', 1.0], ['CA', 19, 'D', 0.995], ['CA', 20, 'D', 1.0], ['CA', 21, 'R', 0.605], ['CA', 22, 'D', 0.55], ['CA', 23, 'R', 0.96], ['CA', 24, 'D', 0.82], ['CA', 25, 'R', 0.71], ['CA', 26, 'D', 0.99], ['CA', 27, 'D', 0.96], ['CA', 28, 'D', 0.94], ['CA', 29, 'D', 0.795], ['CA', 30, 'D', 1.0], ['CA', 31, 'D', 0.98], ['CA', 32, 'D', 0.925], ['CA', 33, 'D', 1.0], ['CA', 34, 'D', 0.97], ['CA', 35, 'D', 0.985], ['CA', 36, 'D', 0.76], ['CA', 37, 'D', 0.95], ['CA', 38, 'D', 0.88], ['CA', 39, 'R', 0.52], ['CA', 40, 'D', 0.98], ['CA', 41, 'D', 1.0], ['CA', 42, 'R', 1.0], ['CA', 43, 'D', 0.785], ['CA', 44, 'D', 0.515], ['CA', 45, 'R', 0.695], ['CA', 46, 'D', 0.975], ['CA', 47, 'D', 0.995], ['CA', 48, 'D', 0.715], ['CA', 49, 'D', 0.765], ['CA', 50, 'R', 0.71], ['CA', 51, 'D', 0.99], ['CA', 52, 'D', 1.0], ['CA', 53, 'D', 0.99], ['CO', 1, 'D', 1.0], ['CO', 2, 'D', 0.995], ['CO', 3, 'R', 0.63], ['CO', 4, 'R', 0.8], ['CO', 5, 'R', 1.0], ['CO', 6, 'D', 0.54], ['CO', 7, 'D', 1.0], ['CT', 1, 'D', 0.995], ['CT', 2, 'D', 1.0], ['CT', 3, 'D', 1.0], ['CT', 4, 'D', 0.79], ['CT', 5, 'D', 1.0], ['DE', 0, 'D', 0.995], ['FL', 1, 'R', 1.0], ['FL', 2, 'R', 1.0], ['FL', 3, 'R', 1.0], ['FL', 4, 'R', 1.0], ['FL', 5, 'D', 0.99], ['FL', 6, 'D', 0.655], ['FL', 7, 'D', 0.695], ['FL', 8, 'R', 1.0], ['FL', 9, 'D', 0.995], ['FL', 11, 'R', 0.995], ['FL', 12, 'R', 0.96], ['FL', 13, 'D', 0.965], ['FL', 15, 'D', 0.685], ['FL', 16, 'D', 0.805], ['FL', 17, 'R', 0.83], ['FL', 18, 'D', 0.5], ['FL', 19, 'R', 0.955], ['FL', 22, 'D', 1.0], ['FL', 23, 'D', 1.0], ['FL', 25, 'R', 0.51], ['FL', 26, 'R', 0.575], ['FL', 27, 'D', 0.62], ['GA', 1, 'R', 0.98], ['GA', 2, 'D', 1.0], ['GA', 3, 'R', 1.0], ['GA', 4, 'D', 0.995], ['GA', 6, 'R', 0.685], ['GA', 7, 'R', 0.555], ['GA', 9, 'R', 0.97], ['GA', 10, 'R', 0.98], ['GA', 11, 'R', 0.99], ['GA', 12, 'R', 1.0], ['GA', 13, 'D', 1.0], ['GA', 14, 'R', 1.0], ['HI', 1, 'D', 0.955], ['HI', 2, 'D', 0.995], ['ID', 1, 'R', 0.985], ['ID', 2, 'R', 1.0], ['IL', 1, 'D', 0.78], ['IL', 2, 'D', 1.0], ['IL', 3, 'D', 0.98], ['IL', 4, 'D', 0.955], ['IL', 5, 'D', 1.0], ['IL', 6, 'R', 0.635], ['IL', 7, 'D', 0.935], ['IL', 8, 'D', 0.945], ['IL', 9, 'D', 1.0], ['IL', 10, 'D', 0.925], ['IL', 11, 'D', 1.0], ['IL', 12, 'R', 0.55], ['IL', 13, 'D', 0.69], ['IL', 14, 'D', 0.55], ['IL', 15, 'R', 1.0], ['IL', 16, 'R', 1.0], ['IL', 17, 'D', 0.97], ['IL', 18, 'R', 0.98], ['IN', 1, 'D', 0.995], ['IN', 2, 'D', 0.625], ['IN', 3, 'R', 0.89], ['IN', 4, 'R', 0.965], ['IN', 5, 'R', 1.0], ['IN', 6, 'R', 0.93], ['IN', 7, 'D', 1.0], ['IN', 8, 'R', 1.0], ['IN', 9, 'R', 0.515], ['IA', 1, 'R', 0.545], ['IA', 2, 'D', 0.95], ['IA', 3, 'R', 0.695], ['IA', 4, 'R', 0.76], ['KS', 1, 'R', 0.995], ['KS', 2, 'D', 0.685], ['KS', 3, 'R', 0.69], ['KS', 4, 'R', 0.735], ['KY', 1, 'R', 1.0], ['KY', 2, 'R', 0.98], ['KY', 3, 'D', 0.76], ['KY', 4, 'R', 0.775], ['KY', 5, 'R', 0.985], ['KY', 6, 'D', 0.72], ['LA', 1, 'R', 0.965], ['LA', 2, 'D', 0.98], ['LA', 3, 'R', 0.515], ['LA', 4, 'R', 1.0], ['LA', 5, 'R', 0.975], ['LA', 6, 'R', 1.0], ['ME', 1, 'D', 0.99], ['ME', 2, 'R', 0.69], ['MD', 1, 'R', 0.575], ['MD', 2, 'D', 0.995], ['MD', 3, 'D', 1.0], ['MD', 4, 'D', 0.985], ['MD', 5, 'D', 0.95], ['MD', 6, 'D', 0.73], ['MD', 7, 'D', 1.0], ['MD', 8, 'D', 1.0], ['MA', 2, 'D', 1.0], ['MA', 3, 'D', 0.655], ['MA', 5, 'D', 0.985], ['MA', 6, 'D', 0.84], ['MA', 9, 'D', 0.9], ['MI', 1, 'R', 0.8], ['MI', 2, 'R', 0.77], ['MI', 3, 'R', 0.99], ['MI', 4, 'R', 1.0], ['MI', 5, 'D', 1.0], ['MI', 6, 'R', 0.615], ['MI', 7, 'R', 0.575], ['MI', 8, 'D', 0.645], ['MI', 9, 'D', 1.0], ['MI', 10, 'R', 0.95], ['MI', 11, 'D', 0.695], ['MI', 12, 'D', 1.0], ['MI', 13, 'D', 0.74], ['MN', 1, 'R', 0.76], ['MN', 2, 'R', 0.66], ['MN', 3, 'R', 0.925], ['MN', 4, 'D', 0.99], ['MN', 5, 'D', 0.89], ['MN', 6, 'R', 0.995], ['MN', 7, 'D', 0.995], ['MN', 8, 'R', 0.765], ['MS', 1, 'R', 1.0], ['MS', 2, 'D', 1.0], ['MS', 3, 'R', 0.795], ['MS', 4, 'R', 0.995], ['MO', 1, 'D', 0.975], ['MO', 2, 'R', 0.62], ['MO', 3, 'R', 1.0], ['MO', 4, 'R', 0.995], ['MO', 5, 'D', 1.0], ['MO', 6, 'R', 0.995], ['MO', 7, 'R', 1.0], ['MO', 8, 'R', 0.995], ['MT', 0, 'R', 0.645], ['NE', 1, 'R', 1.0], ['NE', 2, 'R', 0.57], ['NE', 3, 'R', 0.975], ['NV', 1, 'D', 0.98], ['NV', 2, 'R', 1.0], ['NV', 3, 'D', 0.62], ['NV', 4, 'D', 0.635], ['NH', 1, 'R', 0.55], ['NH', 2, 'D', 0.925], ['NJ', 1, 'D', 1.0], ['NJ', 2, 'D', 0.805], ['NJ', 3, 'R', 0.745], ['NJ', 4, 'R', 0.725], ['NJ', 5, 'D', 0.95], ['NJ', 6, 'D', 0.995], ['NJ', 7, 'D', 0.645], ['NJ', 8, 'D', 0.915], ['NJ', 10, 'D', 0.98], ['NJ', 11, 'D', 0.525], ['NJ', 12, 'D', 0.995], ['NM', 1, 'D', 0.88], ['NM', 2, 'D', 0.71], ['NM', 3, 'D', 0.885], ['NY', 1, 'R', 0.775], ['NY', 2, 'R', 0.53], ['NY', 3, 'D', 0.995], ['NY', 4, 'D', 1.0], ['NY', 6, 'D', 1.0], ['NY', 7, 'D', 0.585], ['NY', 8, 'D', 1.0], ['NY', 9, 'D', 1.0], ['NY', 10, 'D', 1.0], ['NY', 11, 'D', 0.815], ['NY', 12, 'D', 0.96], ['NY', 13, 'D', 0.99], ['NY', 14, 'D', 0.605], ['NY', 15, 'D', 0.64], ['NY', 17, 'D', 1.0], ['NY', 18, 'D', 0.945], ['NY', 19, 'R', 0.775], ['NY', 20, 'D', 0.995], ['NY', 21, 'R', 0.615], ['NY', 22, 'R', 0.74], ['NY', 23, 'R', 0.63], ['NY', 24, 'R', 0.64], ['NY', 25, 'D', 0.635], ['NY', 26, 'D', 1.0], ['NY', 27, 'D', 0.635], ['NC', 1, 'D', 0.995], ['NC', 2, 'R', 0.77], ['NC', 4, 'D', 1.0], ['NC', 5, 'R', 0.995], ['NC', 6, 'R', 1.0], ['NC', 7, 'R', 0.795], ['NC', 8, 'R', 0.955], ['NC', 9, 'D', 0.715], ['NC', 10, 'R', 0.955], ['NC', 11, 'R', 1.0], ['NC', 12, 'D', 0.98], ['NC', 13, 'D', 0.51], ['ND', 0, 'R', 0.78], ['OH', 1, 'D', 0.63], ['OH', 2, 'R', 0.945], ['OH', 3, 'D', 1.0], ['OH', 4, 'R', 0.93], ['OH', 5, 'R', 1.0], ['OH', 6, 'R', 0.995], ['OH', 7, 'D', 0.555], ['OH', 8, 'R', 1.0], ['OH', 9, 'D', 1.0], ['OH', 10, 'R', 0.815], ['OH', 11, 'D', 1.0], ['OH', 12, 'D', 0.74], ['OH', 13, 'D', 1.0], ['OH', 14, 'R', 0.515], ['OH', 15, 'D', 0.555], ['OH', 16, 'R', 0.98], ['OK', 1, 'R', 0.88], ['OK', 2, 'R', 1.0], ['OK', 3, 'R', 1.0], ['OK', 4, 'R', 1.0], ['OK', 5, 'R', 0.67], ['OR', 1, 'D', 1.0], ['OR', 2, 'R', 0.615], ['OR', 3, 'D', 0.8], ['OR', 4, 'D', 1.0], ['OR', 5, 'D', 1.0], ['PA', 1, 'R', 0.565], ['PA', 2, 'D', 1.0], ['PA', 3, 'D', 0.935], ['PA', 4, 'D', 0.615], ['PA', 5, 'D', 0.73], ['PA', 6, 'D', 0.875], ['PA', 7, 'R', 0.93], ['PA', 8, 'D', 0.65], ['PA', 9, 'R', 0.84], ['PA', 10, 'R', 0.825], ['PA', 11, 'R', 0.615], ['PA', 12, 'R', 0.995], ['PA', 13, 'R', 0.805], ['PA', 14, 'R', 0.67], ['PA', 15, 'R', 0.525], ['PA', 16, 'R', 0.61], ['PA', 17, 'D', 0.61], ['RI', 1, 'D', 1.0], ['RI', 2, 'D', 0.99], ['SC', 1, 'R', 0.575], ['SC', 2, 'R', 0.995], ['SC', 3, 'R', 0.99], ['SC', 4, 'R', 0.67], ['SC', 5, 'D', 0.685], ['SC', 6, 'D', 0.99], ['SC', 7, 'R', 1.0], ['SD', 0, 'D', 0.54], ['TN', 1, 'R', 0.95], ['TN', 2, 'R', 0.99], ['TN', 3, 'R', 0.975], ['TN', 4, 'R', 0.74], ['TN', 5, 'D', 0.955], ['TN', 6, 'R', 0.925], ['TN', 7, 'R', 0.945], ['TN', 8, 'R', 0.995], ['TN', 9, 'D', 0.845], ['TX', 1, 'R', 1.0], ['TX', 2, 'D', 0.655], ['TX', 3, 'R', 0.915], ['TX', 4, 'R', 0.995], ['TX', 6, 'R', 0.58], ['TX', 7, 'D', 0.63], ['TX', 8, 'R', 0.895], ['TX', 9, 'D', 0.645], ['TX', 10, 'R', 1.0], ['TX', 11, 'R', 0.97], ['TX', 12, 'R', 1.0], ['TX', 13, 'R', 1.0], ['TX', 14, 'R', 0.995], ['TX', 15, 'D', 0.975], ['TX', 16, 'D', 0.91], ['TX', 17, 'R', 1.0], ['TX', 18, 'D', 1.0], ['TX', 19, 'R', 1.0], ['TX', 20, 'D', 1.0], ['TX', 21, 'D', 0.68], ['TX', 22, 'R', 0.795], ['TX', 23, 'R', 0.52], ['TX', 24, 'R', 0.995], ['TX', 25, 'R', 0.915], ['TX', 26, 'R', 1.0], ['TX', 27, 'R', 1.0], ['TX', 28, 'D', 1.0], ['TX', 29, 'D', 0.785], ['TX', 30, 'D', 0.945], ['TX', 31, 'D', 0.62], ['TX', 32, 'R', 0.77], ['TX', 33, 'D', 0.995], ['TX', 34, 'D', 1.0], ['TX', 35, 'D', 0.995], ['TX', 36, 'R', 0.835], ['UT', 1, 'R', 1.0], ['UT', 2, 'R', 0.995], ['UT', 3, 'R', 1.0], ['UT', 4, 'R', 0.575], ['VT', 0, 'D', 1.0], ['VA', 1, 'R', 1.0], ['VA', 2, 'R', 0.555], ['VA', 4, 'D', 1.0], ['VA', 5, 'D', 0.655], ['VA', 6, 'R', 0.945], ['VA', 7, 'D', 0.62], ['VA', 8, 'D', 1.0], ['VA', 9, 'R', 0.81], ['VA', 10, 'R', 0.515], ['VA', 11, 'D', 0.88], ['WA', 1, 'D', 0.98], ['WA', 2, 'D', 1.0], ['WA', 3, 'D', 0.56], ['WA', 4, 'R', 0.98], ['WA', 5, 'R', 0.68], ['WA', 6, 'D', 0.985], ['WA', 7, 'D', 0.98], ['WA', 8, 'D', 0.6], ['WA', 9, 'D', 0.925], ['WA', 10, 'D', 1.0], ['WV', 1, 'R', 1.0], ['WV', 2, 'R', 0.96], ['WV', 3, 'D', 0.525], ['WI', 1, 'D', 0.535], ['WI', 3, 'D', 0.95], ['WI', 4, 'D', 0.995], ['WI', 5, 'R', 0.94], ['WI', 6, 'D', 0.565], ['WI', 7, 'R', 0.935], ['WI', 8, 'R', 0.995], ['WY', 0, 'R', 1.0]]\n"
     ]
    }
   ],
   "source": [
    "what = pandas.read_csv(\"2018erthush.csv\")\n",
    "\n",
    "dfident = what[['state', 'district', 'partyLEFT', 'partyRIGHT']]\n",
    "df1 = what[['Party_Previous_Vote_ShareLEFT', 'IncumbentLEFT', 'RaisedLEFT',\n",
    "            'SpentLEFT', 'Party_Previous_Vote_ShareRIGHT', 'IncumbentRIGHT', 'RaisedRIGHT', 'SpentRIGHT']]\n",
    "df2 = what[['won']]\n",
    "\n",
    "\n",
    "d1 = df1.fillna(0) \n",
    "d2 = df2.fillna(0)\n",
    "\n",
    "\n",
    "trainingx = d1\n",
    "# print(trainingx)\n",
    "\n",
    "trainingy = d2\n",
    "\n",
    "traininganswers = clf.predict(trainingx)\n",
    "probanswers = clf.predict_proba(trainingx)\n",
    "\n",
    "zedata = pandas.DataFrame(traininganswers, columns=['won'])\n",
    "probdata = pandas.DataFrame(probanswers, columns=['probof0', 'probof1'])\n",
    "\n",
    "# print(traininganswers)\n",
    "identwinners = dfident.join(zedata, lsuffix = 'SpentRIGHT')\n",
    "\n",
    "\n",
    "problist = probdata.values.tolist()\n",
    "print(problist)\n",
    "print(\"look up\")\n",
    "\n",
    "# print(identwinners)\n",
    "\n",
    "# realanswers = np.ravel(trainingy)\n",
    "\n",
    "lister = identwinners.values.tolist()\n",
    "# print(lister)\n",
    "\n",
    "for x in range(0,len(lister)):\n",
    "    if lister[x][4] == 0:\n",
    "        new = lister[x]\n",
    "        new.pop()\n",
    "        new.pop()\n",
    "        lister[x] = new\n",
    "        maxi = max(problist[x])\n",
    "        lister[x].append(maxi)\n",
    "        if lister[x][2] == 'REP':\n",
    "            lister[x][2] = 'R'\n",
    "        elif lister[x][2] == 'DEM' or lister[x][2] == 'DFL':\n",
    "            lister[x][2] = 'D'\n",
    "        else:\n",
    "            print(\"\\n \\n \\n \\n ???? \\n \\n \\n ???? FAIL \\n \\n \\n\")\n",
    "            print(lister[x])\n",
    "    elif lister[x][4] == 1:\n",
    "        new = lister[x]\n",
    "        new.pop()\n",
    "        new.pop(2)\n",
    "        lister[x] = new\n",
    "        maxi = max(problist[x])\n",
    "        lister[x].append(maxi)\n",
    "        if lister[x][2] == 'REP':\n",
    "            lister[x][2] = 'R'\n",
    "        elif lister[x][2] == 'DEM' or lister[x][2] == 'DFL':\n",
    "            lister[x][2] = 'D'\n",
    "        else:\n",
    "            print(\"\\n \\n \\n \\n ???? \\n \\n \\n ???? FAIL \\n \\n \\n\")\n",
    "            print(lister[x])\n",
    "    else:\n",
    "        print(\"#### \\n \\n \\n \\n ##### \\n \\n \\n ### FAIL \\n \\n \\n\")\n",
    "\n",
    "print(lister)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# print(\"Below is our training data analysis\")\n",
    "# summer = sum(realanswers)\n",
    "# print(summer)\n",
    "\n",
    "# numcorrect = np.dot(realanswers, traininganswers)\n",
    "\n",
    "# print(numcorrect)\n",
    "# print(str(numcorrect/summer) + \" is our training accuracy\")\n",
    "\n",
    "# print(\"\\n \\n \\n\")\n",
    "\n",
    "\n",
    "# # testing score\n",
    "# score = metrics.f1_score(trainingy, traininganswers, pos_label=list(set(trainingy)))\n",
    "# training score\n",
    "# score_test = metrics.f1_score(testingy, testinganswers, pos_label=list(set(testingy)))\n",
    "\n",
    "# print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pandas.read_csv(\"newbforec.csv\")\n",
    "    \n",
    "cutdown = comparison[['state', 'district', 'party']]\n",
    "doneso = cutdown.values.tolist()\n",
    "\n",
    "\n",
    "uncontestedones = pandas.read_csv(\"uncontestedthush.csv\")\n",
    "    \n",
    "justrelevant = uncontestedones[['state', 'district', 'party', 'prob']]\n",
    "\n",
    "oops = justrelevant.values.tolist()\n",
    "\n",
    "\n",
    "masterlist = lister + oops\n",
    "\n",
    "#### uncomment below for nate silver comparison\n",
    "\n",
    "# tempor = list(lister)\n",
    "# # print(tempor)\n",
    "# for x in range(0,len(tempor)):\n",
    "#     salut = tempor[x]\n",
    "#     salut.pop()\n",
    "#     tempor[x] = salut\n",
    "    \n",
    "# tempty = list(oops)\n",
    "# # print(tempor)\n",
    "# for x in range(0,len(tempty)):\n",
    "#     salut = tempty[x]\n",
    "#     salut.pop()\n",
    "#     tempty[x] = salut\n",
    "    \n",
    "# theholygrail = tempor + tempty\n",
    "    \n",
    "# # print(tempor)\n",
    "\n",
    "# summy = 0\n",
    "# tote = 0\n",
    "# for x in doneso:\n",
    "#     if x in theholygrail:\n",
    "#         summy += 1\n",
    "#         tote += 1\n",
    "#     else:\n",
    "#         tote +=1\n",
    "        \n",
    "# print(summy,tote, summy/tote)\n",
    "\n",
    "# print(probdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AL', 1, 'R', 0.9506425735488648], ['AL', 2, 'R', 0.9409998053529519], ['AL', 3, 'R', 0.9596775167177857], ['AL', 4, 'R', 0.9506425735488648], ['AL', 5, 'R', 0.9039799999330197], ['AL', 6, 'R', 0.9596775167177857], ['AK', 0, 'R', 0.8591086104145983], ['AZ', 1, 'D', 0.9410619629210711], ['AZ', 2, 'R', 0.5730803030865039], ['AZ', 3, 'D', 0.9506425735488648], ['AZ', 4, 'R', 0.9252526722792392], ['AZ', 5, 'UNK', 0.6423763833457679], ['AZ', 6, 'R', 0.955182097525357], ['AZ', 8, 'R', 0.691872166768896], ['AZ', 9, 'D', 0.5938177088283766], ['AR', 1, 'R', 0.9506425735488648], ['AR', 2, 'R', 0.745475376453901], ['AR', 3, 'R', 0.9596775167177857], ['AR', 4, 'R', 0.9596775167177857], ['CA', 1, 'R', 0.8094141422510884], ['CA', 2, 'D', 0.9506425735488648], ['CA', 3, 'D', 0.949553092349436], ['CA', 4, 'R', 0.6810002961532168], ['CA', 5, 'D', 0.9566522990677611], ['CA', 6, 'D', 0.7049097039557818], ['CA', 7, 'D', 0.8912788220789295], ['CA', 8, 'R', 0.6667369528941579], ['CA', 9, 'D', 0.952889508576349], ['CA', 10, 'R', 0.6751257910073715], ['CA', 11, 'D', 0.9506425735488648], ['CA', 12, 'D', 0.9385415441633023], ['CA', 13, 'D', 0.9488864702148553], ['CA', 14, 'D', 0.9596775167177857], ['CA', 15, 'D', 0.9462358233431992], ['CA', 16, 'D', 0.8323334179458829], ['CA', 17, 'D', 0.9487153567779599], ['CA', 18, 'D', 0.9506425735488648], ['CA', 19, 'D', 0.9596775167177857], ['CA', 20, 'D', 0.9596775167177857], ['CA', 21, 'R', 0.7276044141260752], ['CA', 22, 'R', 0.7178356551485681], ['CA', 23, 'R', 0.9433111765614095], ['CA', 24, 'D', 0.7930463024753374], ['CA', 25, 'R', 0.7148242625549408], ['CA', 26, 'D', 0.9482788450787534], ['CA', 27, 'D', 0.6870776601302073], ['CA', 28, 'D', 0.9433111765614095], ['CA', 29, 'D', 0.8382567227386286], ['CA', 30, 'D', 0.9596775167177857], ['CA', 31, 'D', 0.8361543305870969], ['CA', 32, 'D', 0.8462032599937378], ['CA', 33, 'D', 0.9506425735488648], ['CA', 34, 'D', 0.9506425735488648], ['CA', 35, 'D', 0.9506425735488648], ['CA', 36, 'D', 0.8172237979977425], ['CA', 37, 'D', 0.8460850756919668], ['CA', 38, 'D', 0.8382567227386286], ['CA', 39, 'R', 0.5847019415467071], ['CA', 40, 'D', 0.9536680228900702], ['CA', 41, 'D', 0.9596775167177857], ['CA', 42, 'R', 0.9506425735488648], ['CA', 43, 'D', 0.7994051997621152], ['CA', 44, 'D', 0.5649265793534418], ['CA', 45, 'R', 0.7133402359561392], ['CA', 46, 'D', 0.9506425735488648], ['CA', 47, 'D', 0.9596775167177857], ['CA', 48, 'R', 0.6812704452296898], ['CA', 49, 'D', 0.5093566363117171], ['CA', 50, 'R', 0.7265777736053157], ['CA', 51, 'D', 0.9596775167177857], ['CA', 52, 'D', 0.9428060498249579], ['CA', 53, 'D', 0.9086957713999366], ['CO', 1, 'D', 0.9506425735488648], ['CO', 2, 'D', 0.8460850756919668], ['CO', 3, 'R', 0.747778274380489], ['CO', 4, 'R', 0.8358009555091465], ['CO', 5, 'R', 0.9506425735488648], ['CO', 6, 'R', 0.6786816561839047], ['CO', 7, 'D', 0.9540912556666119], ['CT', 1, 'D', 0.9488864702148553], ['CT', 2, 'D', 0.9506425735488648], ['CT', 3, 'D', 0.9596775167177857], ['CT', 4, 'D', 0.8411273507280361], ['CT', 5, 'D', 0.8460850756919668], ['DE', 0, 'D', 0.8881731111109313], ['FL', 1, 'R', 0.9506425735488648], ['FL', 2, 'R', 0.9506425735488648], ['FL', 3, 'R', 0.955182097525357], ['FL', 4, 'R', 0.921988551886785], ['FL', 5, 'D', 0.9572457645928121], ['FL', 6, 'R', 0.5786990012139475], ['FL', 7, 'D', 0.7039366059835472], ['FL', 8, 'R', 0.9596775167177857], ['FL', 9, 'D', 0.952889508576349], ['FL', 11, 'R', 0.9541263228932074], ['FL', 12, 'R', 0.8910738496608304], ['FL', 13, 'D', 0.8714302100405233], ['FL', 15, 'R', 0.5258561800992028], ['FL', 16, 'R', 0.6863801755259538], ['FL', 17, 'R', 0.8082769748406673], ['FL', 18, 'R', 0.7195345078510887], ['FL', 19, 'R', 0.9286790378704868], ['FL', 22, 'D', 0.9506425735488648], ['FL', 23, 'D', 0.9537317640750236], ['FL', 25, 'R', 0.5847019415467071], ['FL', 26, 'R', 0.6786816561839047], ['FL', 27, 'R', 0.6287272191165139], ['GA', 1, 'R', 0.9506425735488648], ['GA', 2, 'D', 0.9506425735488648], ['GA', 3, 'R', 0.9596775167177857], ['GA', 4, 'D', 0.9240769043852485], ['GA', 6, 'R', 0.8112154698792748], ['GA', 7, 'R', 0.729475955454855], ['GA', 9, 'R', 0.9506425735488648], ['GA', 10, 'R', 0.9506425735488648], ['GA', 11, 'R', 0.956558075018181], ['GA', 12, 'R', 0.9506425735488648], ['GA', 13, 'D', 0.9596775167177857], ['GA', 14, 'R', 0.9596775167177857], ['HI', 1, 'D', 0.8113421170845427], ['HI', 2, 'D', 0.9596775167177857], ['ID', 1, 'R', 0.8460850756919668], ['ID', 2, 'R', 0.9506425735488648], ['IL', 1, 'D', 0.9097344424203855], ['IL', 2, 'D', 0.9506425735488648], ['IL', 3, 'D', 0.9556230990833341], ['IL', 4, 'D', 0.8460850756919668], ['IL', 5, 'D', 0.9596775167177857], ['IL', 6, 'R', 0.7133402359561392], ['IL', 7, 'D', 0.9364830744060272], ['IL', 8, 'D', 0.9435022328362141], ['IL', 9, 'D', 0.9596775167177857], ['IL', 10, 'D', 0.9143924000799207], ['IL', 11, 'D', 0.9465227417447438], ['IL', 12, 'R', 0.6812704452296898], ['IL', 13, 'R', 0.6797711373833336], ['IL', 14, 'R', 0.7253715443642167], ['IL', 15, 'R', 0.9596775167177857], ['IL', 16, 'R', 0.9596775167177857], ['IL', 17, 'D', 0.9385415441633023], ['IL', 18, 'R', 0.9385415441633023], ['IN', 1, 'D', 0.9596775167177857], ['IN', 2, 'R', 0.5847019415467071], ['IN', 3, 'R', 0.8514999805158852], ['IN', 4, 'R', 0.8218542152308604], ['IN', 5, 'R', 0.9506425735488648], ['IN', 6, 'R', 0.8382567227386286], ['IN', 7, 'D', 0.9506425735488648], ['IN', 8, 'R', 0.9506425735488648], ['IN', 9, 'R', 0.7242820631647879], ['IA', 1, 'R', 0.6739912213071432], ['IA', 2, 'D', 0.878881214961562], ['IA', 3, 'R', 0.7249210492297132], ['IA', 4, 'R', 0.7638554795196704], ['KS', 1, 'R', 0.9596775167177857], ['KS', 2, 'R', 0.5517833520103916], ['KS', 3, 'R', 0.7175649487291895], ['KS', 4, 'R', 0.7731228306636753], ['KY', 1, 'R', 0.915457230820008], ['KY', 2, 'R', 0.9506425735488648], ['KY', 3, 'D', 0.8121188325257775], ['KY', 4, 'R', 0.8906214197390204], ['KY', 5, 'R', 0.9506425735488648], ['KY', 6, 'R', 0.6797711373833336], ['LA', 1, 'R', 0.9433111765614095], ['LA', 2, 'D', 0.9506425735488648], ['LA', 3, 'R', 0.7069763609565625], ['LA', 4, 'R', 0.9506425735488648], ['LA', 5, 'R', 0.9506425735488648], ['LA', 6, 'R', 0.9559828316937289], ['ME', 1, 'D', 0.952889508576349], ['ME', 2, 'R', 0.7169358695646274], ['MD', 1, 'R', 0.7480553019821856], ['MD', 2, 'D', 0.9585054031171008], ['MD', 3, 'D', 0.9596775167177857], ['MD', 4, 'D', 0.9596775167177857], ['MD', 5, 'D', 0.9433111765614095], ['MD', 6, 'D', 0.6505701578807609], ['MD', 7, 'D', 0.9506425735488648], ['MD', 8, 'D', 0.9506425735488648], ['MA', 2, 'D', 0.9596775167177857], ['MA', 3, 'D', 0.6377653956079484], ['MA', 5, 'D', 0.9506425735488648], ['MA', 6, 'D', 0.9385415441633023], ['MA', 9, 'D', 0.856859364969949], ['MI', 1, 'R', 0.8053565732918784], ['MI', 2, 'R', 0.8404665825755984], ['MI', 3, 'R', 0.9556521950534297], ['MI', 4, 'R', 0.9585054031171008], ['MI', 5, 'D', 0.9506425735488648], ['MI', 6, 'R', 0.7529192277273964], ['MI', 7, 'R', 0.7447772962397144], ['MI', 8, 'R', 0.6812704452296898], ['MI', 9, 'D', 0.8460850756919668], ['MI', 10, 'R', 0.956558075018181], ['MI', 11, 'R', 0.6378429863981835], ['MI', 12, 'D', 0.9506425735488648], ['MI', 13, 'D', 0.5600817848351369], ['MN', 1, 'R', 0.7997652216143328], ['MN', 2, 'R', 0.6163727417954237], ['MN', 3, 'R', 0.9374520629638735], ['MN', 4, 'D', 0.6627170905818172], ['MN', 5, 'R', 0.5369139988888894], ['MN', 6, 'R', 0.9506425735488648], ['MN', 7, 'D', 0.9278340934545071], ['MN', 8, 'R', 0.810320969862261], ['MS', 1, 'R', 0.9506425735488648], ['MS', 2, 'D', 0.9596775167177857], ['MS', 3, 'R', 0.8179714912502822], ['MS', 4, 'R', 0.9596775167177857], ['MO', 1, 'D', 0.9506425735488648], ['MO', 2, 'R', 0.7274097876983657], ['MO', 3, 'R', 0.9559828316937289], ['MO', 4, 'R', 0.9506425735488648], ['MO', 5, 'D', 0.949553092349436], ['MO', 6, 'R', 0.9506425735488648], ['MO', 7, 'R', 0.9506425735488648], ['MO', 8, 'R', 0.9596775167177857], ['MT', 0, 'R', 0.6293051602476322], ['NE', 1, 'R', 0.9506425735488648], ['NE', 2, 'R', 0.693457997917461], ['NE', 3, 'R', 0.9506425735488648], ['NV', 1, 'D', 0.9585054031171008], ['NV', 2, 'R', 0.949553092349436], ['NV', 3, 'D', 0.540733547798934], ['NV', 4, 'D', 0.5181982410351502], ['NH', 1, 'R', 0.5051012017558189], ['NH', 2, 'D', 0.7882260225515099], ['NJ', 1, 'D', 0.9585054031171008], ['NJ', 2, 'D', 0.5808145342608322], ['NJ', 3, 'R', 0.7133402359561392], ['NJ', 4, 'R', 0.7731228306636753], ['NJ', 5, 'D', 0.8850972291282208], ['NJ', 6, 'D', 0.9482788450787534], ['NJ', 7, 'R', 0.6884189570993557], ['NJ', 8, 'D', 0.9333636327064225], ['NJ', 10, 'D', 0.9536680228900702], ['NJ', 11, 'R', 0.5481305675810223], ['NJ', 12, 'D', 0.9506425735488648], ['NM', 1, 'D', 0.8199979208916695], ['NM', 2, 'R', 0.6184978033463949], ['NM', 3, 'D', 0.8443289723579571], ['NY', 1, 'R', 0.7162648827379289], ['NY', 2, 'R', 0.7448608940388892], ['NY', 3, 'D', 0.9153970590082292], ['NY', 4, 'D', 0.955182097525357], ['NY', 6, 'D', 0.9596775167177857], ['NY', 7, 'D', 0.5599698752398499], ['NY', 8, 'D', 0.9506425735488648], ['NY', 9, 'D', 0.9596775167177857], ['NY', 10, 'D', 0.9596775167177857], ['NY', 11, 'R', 0.6797711373833336], ['NY', 12, 'D', 0.9541727656330007], ['NY', 13, 'D', 0.9596775167177857], ['NY', 14, 'D', 0.6760868931888653], ['NY', 15, 'D', 0.8575566354921285], ['NY', 17, 'D', 0.9596775167177857], ['NY', 18, 'D', 0.9374520629638735], ['NY', 19, 'R', 0.7128814519301756], ['NY', 20, 'D', 0.9506425735488648], ['NY', 21, 'R', 0.7804973732726548], ['NY', 22, 'R', 0.7258425854406315], ['NY', 23, 'R', 0.8239798034589163], ['NY', 24, 'R', 0.7467175440829156], ['NY', 25, 'D', 0.6447880797576834], ['NY', 26, 'D', 0.9506425735488648], ['NY', 27, 'D', 0.5443664143665239], ['NC', 1, 'D', 0.9596775167177857], ['NC', 2, 'R', 0.8310856591205195], ['NC', 4, 'D', 0.9596775167177857], ['NC', 5, 'R', 0.9514874125013002], ['NC', 6, 'R', 0.9506425735488648], ['NC', 7, 'R', 0.8322934378153642], ['NC', 8, 'R', 0.8822890554943235], ['NC', 9, 'R', 0.6287272191165139], ['NC', 10, 'R', 0.9433111765614095], ['NC', 11, 'R', 0.9506425735488648], ['NC', 12, 'D', 0.9456371367964183], ['NC', 13, 'R', 0.6799108149537879], ['ND', 0, 'R', 0.612468357040514], ['OH', 1, 'R', 0.6901160634348864], ['OH', 2, 'R', 0.8923842306632332], ['OH', 3, 'D', 0.9506425735488648], ['OH', 4, 'R', 0.8617872348056004], ['OH', 5, 'R', 0.9596775167177857], ['OH', 6, 'R', 0.9488864702148553], ['OH', 7, 'R', 0.6447111567613617], ['OH', 8, 'R', 0.9506425735488648], ['OH', 9, 'D', 0.9506425735488648], ['OH', 10, 'R', 0.7786648593021448], ['OH', 11, 'D', 0.9506425735488648], ['OH', 12, 'R', 0.687752334964775], ['OH', 13, 'D', 0.9596775167177857], ['OH', 14, 'R', 0.7465648576533299], ['OH', 15, 'R', 0.783449470480432], ['OH', 16, 'R', 0.8460850756919668], ['OK', 1, 'R', 0.8199979208916695], ['OK', 2, 'R', 0.9506425735488648], ['OK', 3, 'R', 0.9506425735488648], ['OK', 4, 'R', 0.9596775167177857], ['OK', 5, 'R', 0.7500995773795619], ['OR', 1, 'D', 0.9506425735488648], ['OR', 2, 'R', 0.8255505758695553], ['OR', 3, 'D', 0.6930664586690951], ['OR', 4, 'D', 0.949553092349436], ['OR', 5, 'D', 0.9517986667176036], ['PA', 1, 'R', 0.5397191065475352], ['PA', 2, 'D', 0.9506425735488648], ['PA', 3, 'D', 0.7793038019046118], ['PA', 4, 'R', 0.5858241885958847], ['PA', 5, 'R', 0.5814556337245427], ['PA', 6, 'D', 0.5989204822728877], ['PA', 7, 'R', 0.8416365908434819], ['PA', 8, 'D', 0.5730404726614835], ['PA', 9, 'R', 0.7608018809778639], ['PA', 10, 'R', 0.7747416327029665], ['PA', 11, 'R', 0.7477458373042194], ['PA', 12, 'R', 0.955182097525357], ['PA', 13, 'R', 0.5989204822728877], ['PA', 14, 'D', 0.6366960507533427], ['PA', 15, 'R', 0.8327205090636502], ['PA', 16, 'R', 0.756374259314707], ['PA', 17, 'R', 0.5262790918353628], ['RI', 1, 'D', 0.9506425735488648], ['RI', 2, 'D', 0.9585054031171008], ['SC', 1, 'R', 0.6561923557596931], ['SC', 2, 'R', 0.9585054031171008], ['SC', 3, 'R', 0.9506425735488648], ['SC', 4, 'R', 0.5548286123469013], ['SC', 5, 'R', 0.6943613731073952], ['SC', 6, 'D', 0.9488864702148553], ['SC', 7, 'R', 0.9506425735488648], ['SD', 0, 'R', 0.5165108227422079], ['TN', 1, 'R', 0.9270970029946706], ['TN', 2, 'R', 0.8460850756919668], ['TN', 3, 'R', 0.9456371367964183], ['TN', 4, 'R', 0.7970750905801168], ['TN', 5, 'D', 0.9473276667159218], ['TN', 6, 'R', 0.8382567227386286], ['TN', 7, 'R', 0.8108619052766545], ['TN', 8, 'R', 0.9488864702148553], ['TN', 9, 'D', 0.9140729979348151], ['TX', 1, 'R', 0.9506425735488648], ['TX', 2, 'D', 0.7321420984525554], ['TX', 3, 'R', 0.8199979208916695], ['TX', 4, 'R', 0.9596775167177857], ['TX', 6, 'R', 0.5999408077847098], ['TX', 7, 'R', 0.6812704452296898], ['TX', 8, 'R', 0.9385415441633023], ['TX', 9, 'D', 0.7368249474289521], ['TX', 10, 'R', 0.9477969890154264], ['TX', 11, 'R', 0.9411303332090876], ['TX', 12, 'R', 0.9596775167177857], ['TX', 13, 'R', 0.9596775167177857], ['TX', 14, 'R', 0.955182097525357], ['TX', 15, 'D', 0.8366311540910354], ['TX', 16, 'D', 0.8460850756919668], ['TX', 17, 'R', 0.9506425735488648], ['TX', 18, 'D', 0.9506425735488648], ['TX', 19, 'R', 0.9506425735488648], ['TX', 20, 'D', 0.9596775167177857], ['TX', 21, 'R', 0.57338976776447], ['TX', 22, 'R', 0.8184771637102759], ['TX', 23, 'R', 0.668018862934428], ['TX', 24, 'R', 0.949553092349436], ['TX', 25, 'R', 0.888116118266843], ['TX', 26, 'R', 0.9488864702148553], ['TX', 27, 'R', 0.9506425735488648], ['TX', 28, 'D', 0.9506425735488648], ['TX', 29, 'D', 0.8278492910541217], ['TX', 30, 'D', 0.8642381706085555], ['TX', 31, 'R', 0.6799108149537879], ['TX', 32, 'R', 0.7232398353651182], ['TX', 33, 'D', 0.9596775167177857], ['TX', 34, 'D', 0.9596775167177857], ['TX', 35, 'D', 0.9456371367964183], ['TX', 36, 'R', 0.8420083260571264], ['UT', 1, 'R', 0.9506425735488648], ['UT', 2, 'R', 0.9585054031171008], ['UT', 3, 'R', 0.9506425735488648], ['UT', 4, 'R', 0.6786816561839047], ['VT', 0, 'D', 0.8550114325004877], ['VA', 1, 'R', 0.9554556444109575], ['VA', 2, 'R', 0.7229179386588419], ['VA', 4, 'D', 0.952889508576349], ['VA', 5, 'R', 0.5425502816388943], ['VA', 6, 'R', 0.8353699611317302], ['VA', 7, 'R', 0.6831256305449791], ['VA', 8, 'D', 0.9488864702148553], ['VA', 9, 'R', 0.7882062156925436], ['VA', 10, 'R', 0.6751257910073715], ['VA', 11, 'D', 0.8982878969250031], ['WA', 1, 'D', 0.9503483332672702], ['WA', 2, 'D', 0.9506425735488648], ['WA', 3, 'R', 0.7395072173235979], ['WA', 4, 'R', 0.9566277580116427], ['WA', 5, 'R', 0.7133402359561392], ['WA', 6, 'D', 0.9514874125013002], ['WA', 7, 'D', 0.9506425735488648], ['WA', 8, 'R', 0.6378429863981835], ['WA', 9, 'D', 0.5167763185831494], ['WA', 10, 'D', 0.955182097525357], ['WV', 1, 'R', 0.9596775167177857], ['WV', 2, 'R', 0.8862749252092398], ['WV', 3, 'R', 0.6451695097492218], ['WI', 1, 'R', 0.5847019415467071], ['WI', 3, 'D', 0.9465227417447438], ['WI', 4, 'D', 0.9506425735488648], ['WI', 5, 'R', 0.9333636327064225], ['WI', 6, 'R', 0.6799108149537879], ['WI', 7, 'R', 0.9385415441633023], ['WI', 8, 'R', 0.9482788450787534], ['WY', 0, 'R', 0.9506425735488648], ['AL', 7, 'D', 1], ['AZ', 7, 'D', 1], ['FL', 10, 'D', 1], ['FL', 14, 'D', 1], ['FL', 20, 'D', 1], ['FL', 21, 'D', 1], ['FL', 24, 'D', 1], ['GA', 5, 'D', 1], ['GA', 8, 'R', 1], ['MA', 1, 'D', 1], ['MA', 4, 'D', 1], ['MA', 7, 'D', 1], ['MA', 8, 'D', 1], ['MI', 14, 'D', 1], ['NJ', 9, 'D', 1], ['NY', 5, 'D', 1], ['NY', 16, 'D', 1], ['NC', 3, 'R', 1], ['PA', 18, 'D', 1], ['TX', 5, 'R', 1], ['VA', 3, 'D', 1], ['WI', 2, 'D', 1]]\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "print(masterlist)\n",
    "\n",
    "\n",
    "sado = 0\n",
    "\n",
    "meanslist = []\n",
    "for x in masterlist:\n",
    "    if x[2] == 'D':\n",
    "        sado += 1\n",
    "        meanslist.append(x[3])\n",
    "    else:\n",
    "        meanslist.append(1-x[3])\n",
    "\n",
    "print(sado)\n",
    "# print(meanslist)\n",
    "# print(probdata)\n",
    "\n",
    "# probber = probdata.values.tolist()\n",
    "\n",
    "# meanslist = []\n",
    "# for x in probber:\n",
    "#     meanslist.append(x[1])\n",
    "\n",
    "# print(meanslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AL 1', 'R', 0.9506425735488648], ['AL 2', 'R', 0.9409998053529519], ['AL 3', 'R', 0.9596775167177857], ['AL 4', 'R', 0.9506425735488648], ['AL 5', 'R', 0.9039799999330197], ['AL 6', 'R', 0.9596775167177857], ['AK 0', 'R', 0.8591086104145983], ['AZ 1', 'D', 0.9410619629210711], ['AZ 2', 'R', 0.5730803030865039], ['AZ 3', 'D', 0.9506425735488648], ['AZ 4', 'R', 0.9252526722792392], ['AZ 5', 'UNK', 0.6423763833457679], ['AZ 6', 'R', 0.955182097525357], ['AZ 8', 'R', 0.691872166768896], ['AZ 9', 'D', 0.5938177088283766], ['AR 1', 'R', 0.9506425735488648], ['AR 2', 'R', 0.745475376453901], ['AR 3', 'R', 0.9596775167177857], ['AR 4', 'R', 0.9596775167177857], ['CA 1', 'R', 0.8094141422510884], ['CA 2', 'D', 0.9506425735488648], ['CA 3', 'D', 0.949553092349436], ['CA 4', 'R', 0.6810002961532168], ['CA 5', 'D', 0.9566522990677611], ['CA 6', 'D', 0.7049097039557818], ['CA 7', 'D', 0.8912788220789295], ['CA 8', 'R', 0.6667369528941579], ['CA 9', 'D', 0.952889508576349], ['CA 10', 'R', 0.6751257910073715], ['CA 11', 'D', 0.9506425735488648], ['CA 12', 'D', 0.9385415441633023], ['CA 13', 'D', 0.9488864702148553], ['CA 14', 'D', 0.9596775167177857], ['CA 15', 'D', 0.9462358233431992], ['CA 16', 'D', 0.8323334179458829], ['CA 17', 'D', 0.9487153567779599], ['CA 18', 'D', 0.9506425735488648], ['CA 19', 'D', 0.9596775167177857], ['CA 20', 'D', 0.9596775167177857], ['CA 21', 'R', 0.7276044141260752], ['CA 22', 'R', 0.7178356551485681], ['CA 23', 'R', 0.9433111765614095], ['CA 24', 'D', 0.7930463024753374], ['CA 25', 'R', 0.7148242625549408], ['CA 26', 'D', 0.9482788450787534], ['CA 27', 'D', 0.6870776601302073], ['CA 28', 'D', 0.9433111765614095], ['CA 29', 'D', 0.8382567227386286], ['CA 30', 'D', 0.9596775167177857], ['CA 31', 'D', 0.8361543305870969], ['CA 32', 'D', 0.8462032599937378], ['CA 33', 'D', 0.9506425735488648], ['CA 34', 'D', 0.9506425735488648], ['CA 35', 'D', 0.9506425735488648], ['CA 36', 'D', 0.8172237979977425], ['CA 37', 'D', 0.8460850756919668], ['CA 38', 'D', 0.8382567227386286], ['CA 39', 'R', 0.5847019415467071], ['CA 40', 'D', 0.9536680228900702], ['CA 41', 'D', 0.9596775167177857], ['CA 42', 'R', 0.9506425735488648], ['CA 43', 'D', 0.7994051997621152], ['CA 44', 'D', 0.5649265793534418], ['CA 45', 'R', 0.7133402359561392], ['CA 46', 'D', 0.9506425735488648], ['CA 47', 'D', 0.9596775167177857], ['CA 48', 'R', 0.6812704452296898], ['CA 49', 'D', 0.5093566363117171], ['CA 50', 'R', 0.7265777736053157], ['CA 51', 'D', 0.9596775167177857], ['CA 52', 'D', 0.9428060498249579], ['CA 53', 'D', 0.9086957713999366], ['CO 1', 'D', 0.9506425735488648], ['CO 2', 'D', 0.8460850756919668], ['CO 3', 'R', 0.747778274380489], ['CO 4', 'R', 0.8358009555091465], ['CO 5', 'R', 0.9506425735488648], ['CO 6', 'R', 0.6786816561839047], ['CO 7', 'D', 0.9540912556666119], ['CT 1', 'D', 0.9488864702148553], ['CT 2', 'D', 0.9506425735488648], ['CT 3', 'D', 0.9596775167177857], ['CT 4', 'D', 0.8411273507280361], ['CT 5', 'D', 0.8460850756919668], ['DE 0', 'D', 0.8881731111109313], ['FL 1', 'R', 0.9506425735488648], ['FL 2', 'R', 0.9506425735488648], ['FL 3', 'R', 0.955182097525357], ['FL 4', 'R', 0.921988551886785], ['FL 5', 'D', 0.9572457645928121], ['FL 6', 'R', 0.5786990012139475], ['FL 7', 'D', 0.7039366059835472], ['FL 8', 'R', 0.9596775167177857], ['FL 9', 'D', 0.952889508576349], ['FL 11', 'R', 0.9541263228932074], ['FL 12', 'R', 0.8910738496608304], ['FL 13', 'D', 0.8714302100405233], ['FL 15', 'R', 0.5258561800992028], ['FL 16', 'R', 0.6863801755259538], ['FL 17', 'R', 0.8082769748406673], ['FL 18', 'R', 0.7195345078510887], ['FL 19', 'R', 0.9286790378704868], ['FL 22', 'D', 0.9506425735488648], ['FL 23', 'D', 0.9537317640750236], ['FL 25', 'R', 0.5847019415467071], ['FL 26', 'R', 0.6786816561839047], ['FL 27', 'R', 0.6287272191165139], ['GA 1', 'R', 0.9506425735488648], ['GA 2', 'D', 0.9506425735488648], ['GA 3', 'R', 0.9596775167177857], ['GA 4', 'D', 0.9240769043852485], ['GA 6', 'R', 0.8112154698792748], ['GA 7', 'R', 0.729475955454855], ['GA 9', 'R', 0.9506425735488648], ['GA 10', 'R', 0.9506425735488648], ['GA 11', 'R', 0.956558075018181], ['GA 12', 'R', 0.9506425735488648], ['GA 13', 'D', 0.9596775167177857], ['GA 14', 'R', 0.9596775167177857], ['HI 1', 'D', 0.8113421170845427], ['HI 2', 'D', 0.9596775167177857], ['ID 1', 'R', 0.8460850756919668], ['ID 2', 'R', 0.9506425735488648], ['IL 1', 'D', 0.9097344424203855], ['IL 2', 'D', 0.9506425735488648], ['IL 3', 'D', 0.9556230990833341], ['IL 4', 'D', 0.8460850756919668], ['IL 5', 'D', 0.9596775167177857], ['IL 6', 'R', 0.7133402359561392], ['IL 7', 'D', 0.9364830744060272], ['IL 8', 'D', 0.9435022328362141], ['IL 9', 'D', 0.9596775167177857], ['IL 10', 'D', 0.9143924000799207], ['IL 11', 'D', 0.9465227417447438], ['IL 12', 'R', 0.6812704452296898], ['IL 13', 'R', 0.6797711373833336], ['IL 14', 'R', 0.7253715443642167], ['IL 15', 'R', 0.9596775167177857], ['IL 16', 'R', 0.9596775167177857], ['IL 17', 'D', 0.9385415441633023], ['IL 18', 'R', 0.9385415441633023], ['IN 1', 'D', 0.9596775167177857], ['IN 2', 'R', 0.5847019415467071], ['IN 3', 'R', 0.8514999805158852], ['IN 4', 'R', 0.8218542152308604], ['IN 5', 'R', 0.9506425735488648], ['IN 6', 'R', 0.8382567227386286], ['IN 7', 'D', 0.9506425735488648], ['IN 8', 'R', 0.9506425735488648], ['IN 9', 'R', 0.7242820631647879], ['IA 1', 'R', 0.6739912213071432], ['IA 2', 'D', 0.878881214961562], ['IA 3', 'R', 0.7249210492297132], ['IA 4', 'R', 0.7638554795196704], ['KS 1', 'R', 0.9596775167177857], ['KS 2', 'R', 0.5517833520103916], ['KS 3', 'R', 0.7175649487291895], ['KS 4', 'R', 0.7731228306636753], ['KY 1', 'R', 0.915457230820008], ['KY 2', 'R', 0.9506425735488648], ['KY 3', 'D', 0.8121188325257775], ['KY 4', 'R', 0.8906214197390204], ['KY 5', 'R', 0.9506425735488648], ['KY 6', 'R', 0.6797711373833336], ['LA 1', 'R', 0.9433111765614095], ['LA 2', 'D', 0.9506425735488648], ['LA 3', 'R', 0.7069763609565625], ['LA 4', 'R', 0.9506425735488648], ['LA 5', 'R', 0.9506425735488648], ['LA 6', 'R', 0.9559828316937289], ['ME 1', 'D', 0.952889508576349], ['ME 2', 'R', 0.7169358695646274], ['MD 1', 'R', 0.7480553019821856], ['MD 2', 'D', 0.9585054031171008], ['MD 3', 'D', 0.9596775167177857], ['MD 4', 'D', 0.9596775167177857], ['MD 5', 'D', 0.9433111765614095], ['MD 6', 'D', 0.6505701578807609], ['MD 7', 'D', 0.9506425735488648], ['MD 8', 'D', 0.9506425735488648], ['MA 2', 'D', 0.9596775167177857], ['MA 3', 'D', 0.6377653956079484], ['MA 5', 'D', 0.9506425735488648], ['MA 6', 'D', 0.9385415441633023], ['MA 9', 'D', 0.856859364969949], ['MI 1', 'R', 0.8053565732918784], ['MI 2', 'R', 0.8404665825755984], ['MI 3', 'R', 0.9556521950534297], ['MI 4', 'R', 0.9585054031171008], ['MI 5', 'D', 0.9506425735488648], ['MI 6', 'R', 0.7529192277273964], ['MI 7', 'R', 0.7447772962397144], ['MI 8', 'R', 0.6812704452296898], ['MI 9', 'D', 0.8460850756919668], ['MI 10', 'R', 0.956558075018181], ['MI 11', 'R', 0.6378429863981835], ['MI 12', 'D', 0.9506425735488648], ['MI 13', 'D', 0.5600817848351369], ['MN 1', 'R', 0.7997652216143328], ['MN 2', 'R', 0.6163727417954237], ['MN 3', 'R', 0.9374520629638735], ['MN 4', 'D', 0.6627170905818172], ['MN 5', 'R', 0.5369139988888894], ['MN 6', 'R', 0.9506425735488648], ['MN 7', 'D', 0.9278340934545071], ['MN 8', 'R', 0.810320969862261], ['MS 1', 'R', 0.9506425735488648], ['MS 2', 'D', 0.9596775167177857], ['MS 3', 'R', 0.8179714912502822], ['MS 4', 'R', 0.9596775167177857], ['MO 1', 'D', 0.9506425735488648], ['MO 2', 'R', 0.7274097876983657], ['MO 3', 'R', 0.9559828316937289], ['MO 4', 'R', 0.9506425735488648], ['MO 5', 'D', 0.949553092349436], ['MO 6', 'R', 0.9506425735488648], ['MO 7', 'R', 0.9506425735488648], ['MO 8', 'R', 0.9596775167177857], ['MT 0', 'R', 0.6293051602476322], ['NE 1', 'R', 0.9506425735488648], ['NE 2', 'R', 0.693457997917461], ['NE 3', 'R', 0.9506425735488648], ['NV 1', 'D', 0.9585054031171008], ['NV 2', 'R', 0.949553092349436], ['NV 3', 'D', 0.540733547798934], ['NV 4', 'D', 0.5181982410351502], ['NH 1', 'R', 0.5051012017558189], ['NH 2', 'D', 0.7882260225515099], ['NJ 1', 'D', 0.9585054031171008], ['NJ 2', 'D', 0.5808145342608322], ['NJ 3', 'R', 0.7133402359561392], ['NJ 4', 'R', 0.7731228306636753], ['NJ 5', 'D', 0.8850972291282208], ['NJ 6', 'D', 0.9482788450787534], ['NJ 7', 'R', 0.6884189570993557], ['NJ 8', 'D', 0.9333636327064225], ['NJ 10', 'D', 0.9536680228900702], ['NJ 11', 'R', 0.5481305675810223], ['NJ 12', 'D', 0.9506425735488648], ['NM 1', 'D', 0.8199979208916695], ['NM 2', 'R', 0.6184978033463949], ['NM 3', 'D', 0.8443289723579571], ['NY 1', 'R', 0.7162648827379289], ['NY 2', 'R', 0.7448608940388892], ['NY 3', 'D', 0.9153970590082292], ['NY 4', 'D', 0.955182097525357], ['NY 6', 'D', 0.9596775167177857], ['NY 7', 'D', 0.5599698752398499], ['NY 8', 'D', 0.9506425735488648], ['NY 9', 'D', 0.9596775167177857], ['NY 10', 'D', 0.9596775167177857], ['NY 11', 'R', 0.6797711373833336], ['NY 12', 'D', 0.9541727656330007], ['NY 13', 'D', 0.9596775167177857], ['NY 14', 'D', 0.6760868931888653], ['NY 15', 'D', 0.8575566354921285], ['NY 17', 'D', 0.9596775167177857], ['NY 18', 'D', 0.9374520629638735], ['NY 19', 'R', 0.7128814519301756], ['NY 20', 'D', 0.9506425735488648], ['NY 21', 'R', 0.7804973732726548], ['NY 22', 'R', 0.7258425854406315], ['NY 23', 'R', 0.8239798034589163], ['NY 24', 'R', 0.7467175440829156], ['NY 25', 'D', 0.6447880797576834], ['NY 26', 'D', 0.9506425735488648], ['NY 27', 'D', 0.5443664143665239], ['NC 1', 'D', 0.9596775167177857], ['NC 2', 'R', 0.8310856591205195], ['NC 4', 'D', 0.9596775167177857], ['NC 5', 'R', 0.9514874125013002], ['NC 6', 'R', 0.9506425735488648], ['NC 7', 'R', 0.8322934378153642], ['NC 8', 'R', 0.8822890554943235], ['NC 9', 'R', 0.6287272191165139], ['NC 10', 'R', 0.9433111765614095], ['NC 11', 'R', 0.9506425735488648], ['NC 12', 'D', 0.9456371367964183], ['NC 13', 'R', 0.6799108149537879], ['ND 0', 'R', 0.612468357040514], ['OH 1', 'R', 0.6901160634348864], ['OH 2', 'R', 0.8923842306632332], ['OH 3', 'D', 0.9506425735488648], ['OH 4', 'R', 0.8617872348056004], ['OH 5', 'R', 0.9596775167177857], ['OH 6', 'R', 0.9488864702148553], ['OH 7', 'R', 0.6447111567613617], ['OH 8', 'R', 0.9506425735488648], ['OH 9', 'D', 0.9506425735488648], ['OH 10', 'R', 0.7786648593021448], ['OH 11', 'D', 0.9506425735488648], ['OH 12', 'R', 0.687752334964775], ['OH 13', 'D', 0.9596775167177857], ['OH 14', 'R', 0.7465648576533299], ['OH 15', 'R', 0.783449470480432], ['OH 16', 'R', 0.8460850756919668], ['OK 1', 'R', 0.8199979208916695], ['OK 2', 'R', 0.9506425735488648], ['OK 3', 'R', 0.9506425735488648], ['OK 4', 'R', 0.9596775167177857], ['OK 5', 'R', 0.7500995773795619], ['OR 1', 'D', 0.9506425735488648], ['OR 2', 'R', 0.8255505758695553], ['OR 3', 'D', 0.6930664586690951], ['OR 4', 'D', 0.949553092349436], ['OR 5', 'D', 0.9517986667176036], ['PA 1', 'R', 0.5397191065475352], ['PA 2', 'D', 0.9506425735488648], ['PA 3', 'D', 0.7793038019046118], ['PA 4', 'R', 0.5858241885958847], ['PA 5', 'R', 0.5814556337245427], ['PA 6', 'D', 0.5989204822728877], ['PA 7', 'R', 0.8416365908434819], ['PA 8', 'D', 0.5730404726614835], ['PA 9', 'R', 0.7608018809778639], ['PA 10', 'R', 0.7747416327029665], ['PA 11', 'R', 0.7477458373042194], ['PA 12', 'R', 0.955182097525357], ['PA 13', 'R', 0.5989204822728877], ['PA 14', 'D', 0.6366960507533427], ['PA 15', 'R', 0.8327205090636502], ['PA 16', 'R', 0.756374259314707], ['PA 17', 'R', 0.5262790918353628], ['RI 1', 'D', 0.9506425735488648], ['RI 2', 'D', 0.9585054031171008], ['SC 1', 'R', 0.6561923557596931], ['SC 2', 'R', 0.9585054031171008], ['SC 3', 'R', 0.9506425735488648], ['SC 4', 'R', 0.5548286123469013], ['SC 5', 'R', 0.6943613731073952], ['SC 6', 'D', 0.9488864702148553], ['SC 7', 'R', 0.9506425735488648], ['SD 0', 'R', 0.5165108227422079], ['TN 1', 'R', 0.9270970029946706], ['TN 2', 'R', 0.8460850756919668], ['TN 3', 'R', 0.9456371367964183], ['TN 4', 'R', 0.7970750905801168], ['TN 5', 'D', 0.9473276667159218], ['TN 6', 'R', 0.8382567227386286], ['TN 7', 'R', 0.8108619052766545], ['TN 8', 'R', 0.9488864702148553], ['TN 9', 'D', 0.9140729979348151], ['TX 1', 'R', 0.9506425735488648], ['TX 2', 'D', 0.7321420984525554], ['TX 3', 'R', 0.8199979208916695], ['TX 4', 'R', 0.9596775167177857], ['TX 6', 'R', 0.5999408077847098], ['TX 7', 'R', 0.6812704452296898], ['TX 8', 'R', 0.9385415441633023], ['TX 9', 'D', 0.7368249474289521], ['TX 10', 'R', 0.9477969890154264], ['TX 11', 'R', 0.9411303332090876], ['TX 12', 'R', 0.9596775167177857], ['TX 13', 'R', 0.9596775167177857], ['TX 14', 'R', 0.955182097525357], ['TX 15', 'D', 0.8366311540910354], ['TX 16', 'D', 0.8460850756919668], ['TX 17', 'R', 0.9506425735488648], ['TX 18', 'D', 0.9506425735488648], ['TX 19', 'R', 0.9506425735488648], ['TX 20', 'D', 0.9596775167177857], ['TX 21', 'R', 0.57338976776447], ['TX 22', 'R', 0.8184771637102759], ['TX 23', 'R', 0.668018862934428], ['TX 24', 'R', 0.949553092349436], ['TX 25', 'R', 0.888116118266843], ['TX 26', 'R', 0.9488864702148553], ['TX 27', 'R', 0.9506425735488648], ['TX 28', 'D', 0.9506425735488648], ['TX 29', 'D', 0.8278492910541217], ['TX 30', 'D', 0.8642381706085555], ['TX 31', 'R', 0.6799108149537879], ['TX 32', 'R', 0.7232398353651182], ['TX 33', 'D', 0.9596775167177857], ['TX 34', 'D', 0.9596775167177857], ['TX 35', 'D', 0.9456371367964183], ['TX 36', 'R', 0.8420083260571264], ['UT 1', 'R', 0.9506425735488648], ['UT 2', 'R', 0.9585054031171008], ['UT 3', 'R', 0.9506425735488648], ['UT 4', 'R', 0.6786816561839047], ['VT 0', 'D', 0.8550114325004877], ['VA 1', 'R', 0.9554556444109575], ['VA 2', 'R', 0.7229179386588419], ['VA 4', 'D', 0.952889508576349], ['VA 5', 'R', 0.5425502816388943], ['VA 6', 'R', 0.8353699611317302], ['VA 7', 'R', 0.6831256305449791], ['VA 8', 'D', 0.9488864702148553], ['VA 9', 'R', 0.7882062156925436], ['VA 10', 'R', 0.6751257910073715], ['VA 11', 'D', 0.8982878969250031], ['WA 1', 'D', 0.9503483332672702], ['WA 2', 'D', 0.9506425735488648], ['WA 3', 'R', 0.7395072173235979], ['WA 4', 'R', 0.9566277580116427], ['WA 5', 'R', 0.7133402359561392], ['WA 6', 'D', 0.9514874125013002], ['WA 7', 'D', 0.9506425735488648], ['WA 8', 'R', 0.6378429863981835], ['WA 9', 'D', 0.5167763185831494], ['WA 10', 'D', 0.955182097525357], ['WV 1', 'R', 0.9596775167177857], ['WV 2', 'R', 0.8862749252092398], ['WV 3', 'R', 0.6451695097492218], ['WI 1', 'R', 0.5847019415467071], ['WI 3', 'D', 0.9465227417447438], ['WI 4', 'D', 0.9506425735488648], ['WI 5', 'R', 0.9333636327064225], ['WI 6', 'R', 0.6799108149537879], ['WI 7', 'R', 0.9385415441633023], ['WI 8', 'R', 0.9482788450787534], ['WY 0', 'R', 0.9506425735488648], ['AL 7', 'D', 1], ['AZ 7', 'D', 1], ['FL 10', 'D', 1], ['FL 14', 'D', 1], ['FL 20', 'D', 1], ['FL 21', 'D', 1], ['FL 24', 'D', 1], ['GA 5', 'D', 1], ['GA 8', 'R', 1], ['MA 1', 'D', 1], ['MA 4', 'D', 1], ['MA 7', 'D', 1], ['MA 8', 'D', 1], ['MI 14', 'D', 1], ['NJ 9', 'D', 1], ['NY 5', 'D', 1], ['NY 16', 'D', 1], ['NC 3', 'R', 1], ['PA 18', 'D', 1], ['TX 5', 'R', 1], ['VA 3', 'D', 1], ['WI 2', 'D', 1]]\n"
     ]
    }
   ],
   "source": [
    "for x in masterlist:\n",
    "    stringy = x[0] + \" \" + str(x[1])\n",
    "    x.pop(1)\n",
    "    x[0] = stringy\n",
    "\n",
    "print(masterlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AK 0', 'R', 0.8591086104145983], ['AL 1', 'R', 0.9506425735488648], ['AL 2', 'R', 0.9409998053529519], ['AL 3', 'R', 0.9596775167177857], ['AL 4', 'R', 0.9506425735488648], ['AL 5', 'R', 0.9039799999330197], ['AL 6', 'R', 0.9596775167177857], ['AL 7', 'D', 1], ['AR 1', 'R', 0.9506425735488648], ['AR 2', 'R', 0.745475376453901], ['AR 3', 'R', 0.9596775167177857], ['AR 4', 'R', 0.9596775167177857], ['AZ 1', 'D', 0.9410619629210711], ['AZ 2', 'R', 0.5730803030865039], ['AZ 3', 'D', 0.9506425735488648], ['AZ 4', 'R', 0.9252526722792392], ['AZ 5', 'UNK', 0.6423763833457679], ['AZ 6', 'R', 0.955182097525357], ['AZ 7', 'D', 1], ['AZ 8', 'R', 0.691872166768896], ['AZ 9', 'D', 0.5938177088283766], ['CA 1', 'R', 0.8094141422510884], ['CA 10', 'R', 0.6751257910073715], ['CA 11', 'D', 0.9506425735488648], ['CA 12', 'D', 0.9385415441633023], ['CA 13', 'D', 0.9488864702148553], ['CA 14', 'D', 0.9596775167177857], ['CA 15', 'D', 0.9462358233431992], ['CA 16', 'D', 0.8323334179458829], ['CA 17', 'D', 0.9487153567779599], ['CA 18', 'D', 0.9506425735488648], ['CA 19', 'D', 0.9596775167177857], ['CA 2', 'D', 0.9506425735488648], ['CA 20', 'D', 0.9596775167177857], ['CA 21', 'R', 0.7276044141260752], ['CA 22', 'R', 0.7178356551485681], ['CA 23', 'R', 0.9433111765614095], ['CA 24', 'D', 0.7930463024753374], ['CA 25', 'R', 0.7148242625549408], ['CA 26', 'D', 0.9482788450787534], ['CA 27', 'D', 0.6870776601302073], ['CA 28', 'D', 0.9433111765614095], ['CA 29', 'D', 0.8382567227386286], ['CA 3', 'D', 0.949553092349436], ['CA 30', 'D', 0.9596775167177857], ['CA 31', 'D', 0.8361543305870969], ['CA 32', 'D', 0.8462032599937378], ['CA 33', 'D', 0.9506425735488648], ['CA 34', 'D', 0.9506425735488648], ['CA 35', 'D', 0.9506425735488648], ['CA 36', 'D', 0.8172237979977425], ['CA 37', 'D', 0.8460850756919668], ['CA 38', 'D', 0.8382567227386286], ['CA 39', 'R', 0.5847019415467071], ['CA 4', 'R', 0.6810002961532168], ['CA 40', 'D', 0.9536680228900702], ['CA 41', 'D', 0.9596775167177857], ['CA 42', 'R', 0.9506425735488648], ['CA 43', 'D', 0.7994051997621152], ['CA 44', 'D', 0.5649265793534418], ['CA 45', 'R', 0.7133402359561392], ['CA 46', 'D', 0.9506425735488648], ['CA 47', 'D', 0.9596775167177857], ['CA 48', 'R', 0.6812704452296898], ['CA 49', 'D', 0.5093566363117171], ['CA 5', 'D', 0.9566522990677611], ['CA 50', 'R', 0.7265777736053157], ['CA 51', 'D', 0.9596775167177857], ['CA 52', 'D', 0.9428060498249579], ['CA 53', 'D', 0.9086957713999366], ['CA 6', 'D', 0.7049097039557818], ['CA 7', 'D', 0.8912788220789295], ['CA 8', 'R', 0.6667369528941579], ['CA 9', 'D', 0.952889508576349], ['CO 1', 'D', 0.9506425735488648], ['CO 2', 'D', 0.8460850756919668], ['CO 3', 'R', 0.747778274380489], ['CO 4', 'R', 0.8358009555091465], ['CO 5', 'R', 0.9506425735488648], ['CO 6', 'R', 0.6786816561839047], ['CO 7', 'D', 0.9540912556666119], ['CT 1', 'D', 0.9488864702148553], ['CT 2', 'D', 0.9506425735488648], ['CT 3', 'D', 0.9596775167177857], ['CT 4', 'D', 0.8411273507280361], ['CT 5', 'D', 0.8460850756919668], ['DE 0', 'D', 0.8881731111109313], ['FL 1', 'R', 0.9506425735488648], ['FL 10', 'D', 1], ['FL 11', 'R', 0.9541263228932074], ['FL 12', 'R', 0.8910738496608304], ['FL 13', 'D', 0.8714302100405233], ['FL 14', 'D', 1], ['FL 15', 'R', 0.5258561800992028], ['FL 16', 'R', 0.6863801755259538], ['FL 17', 'R', 0.8082769748406673], ['FL 18', 'R', 0.7195345078510887], ['FL 19', 'R', 0.9286790378704868], ['FL 2', 'R', 0.9506425735488648], ['FL 20', 'D', 1], ['FL 21', 'D', 1], ['FL 22', 'D', 0.9506425735488648], ['FL 23', 'D', 0.9537317640750236], ['FL 24', 'D', 1], ['FL 25', 'R', 0.5847019415467071], ['FL 26', 'R', 0.6786816561839047], ['FL 27', 'R', 0.6287272191165139], ['FL 3', 'R', 0.955182097525357], ['FL 4', 'R', 0.921988551886785], ['FL 5', 'D', 0.9572457645928121], ['FL 6', 'R', 0.5786990012139475], ['FL 7', 'D', 0.7039366059835472], ['FL 8', 'R', 0.9596775167177857], ['FL 9', 'D', 0.952889508576349], ['GA 1', 'R', 0.9506425735488648], ['GA 10', 'R', 0.9506425735488648], ['GA 11', 'R', 0.956558075018181], ['GA 12', 'R', 0.9506425735488648], ['GA 13', 'D', 0.9596775167177857], ['GA 14', 'R', 0.9596775167177857], ['GA 2', 'D', 0.9506425735488648], ['GA 3', 'R', 0.9596775167177857], ['GA 4', 'D', 0.9240769043852485], ['GA 5', 'D', 1], ['GA 6', 'R', 0.8112154698792748], ['GA 7', 'R', 0.729475955454855], ['GA 8', 'R', 1], ['GA 9', 'R', 0.9506425735488648], ['HI 1', 'D', 0.8113421170845427], ['HI 2', 'D', 0.9596775167177857], ['IA 1', 'R', 0.6739912213071432], ['IA 2', 'D', 0.878881214961562], ['IA 3', 'R', 0.7249210492297132], ['IA 4', 'R', 0.7638554795196704], ['ID 1', 'R', 0.8460850756919668], ['ID 2', 'R', 0.9506425735488648], ['IL 1', 'D', 0.9097344424203855], ['IL 10', 'D', 0.9143924000799207], ['IL 11', 'D', 0.9465227417447438], ['IL 12', 'R', 0.6812704452296898], ['IL 13', 'R', 0.6797711373833336], ['IL 14', 'R', 0.7253715443642167], ['IL 15', 'R', 0.9596775167177857], ['IL 16', 'R', 0.9596775167177857], ['IL 17', 'D', 0.9385415441633023], ['IL 18', 'R', 0.9385415441633023], ['IL 2', 'D', 0.9506425735488648], ['IL 3', 'D', 0.9556230990833341], ['IL 4', 'D', 0.8460850756919668], ['IL 5', 'D', 0.9596775167177857], ['IL 6', 'R', 0.7133402359561392], ['IL 7', 'D', 0.9364830744060272], ['IL 8', 'D', 0.9435022328362141], ['IL 9', 'D', 0.9596775167177857], ['IN 1', 'D', 0.9596775167177857], ['IN 2', 'R', 0.5847019415467071], ['IN 3', 'R', 0.8514999805158852], ['IN 4', 'R', 0.8218542152308604], ['IN 5', 'R', 0.9506425735488648], ['IN 6', 'R', 0.8382567227386286], ['IN 7', 'D', 0.9506425735488648], ['IN 8', 'R', 0.9506425735488648], ['IN 9', 'R', 0.7242820631647879], ['KS 1', 'R', 0.9596775167177857], ['KS 2', 'R', 0.5517833520103916], ['KS 3', 'R', 0.7175649487291895], ['KS 4', 'R', 0.7731228306636753], ['KY 1', 'R', 0.915457230820008], ['KY 2', 'R', 0.9506425735488648], ['KY 3', 'D', 0.8121188325257775], ['KY 4', 'R', 0.8906214197390204], ['KY 5', 'R', 0.9506425735488648], ['KY 6', 'R', 0.6797711373833336], ['LA 1', 'R', 0.9433111765614095], ['LA 2', 'D', 0.9506425735488648], ['LA 3', 'R', 0.7069763609565625], ['LA 4', 'R', 0.9506425735488648], ['LA 5', 'R', 0.9506425735488648], ['LA 6', 'R', 0.9559828316937289], ['MA 1', 'D', 1], ['MA 2', 'D', 0.9596775167177857], ['MA 3', 'D', 0.6377653956079484], ['MA 4', 'D', 1], ['MA 5', 'D', 0.9506425735488648], ['MA 6', 'D', 0.9385415441633023], ['MA 7', 'D', 1], ['MA 8', 'D', 1], ['MA 9', 'D', 0.856859364969949], ['MD 1', 'R', 0.7480553019821856], ['MD 2', 'D', 0.9585054031171008], ['MD 3', 'D', 0.9596775167177857], ['MD 4', 'D', 0.9596775167177857], ['MD 5', 'D', 0.9433111765614095], ['MD 6', 'D', 0.6505701578807609], ['MD 7', 'D', 0.9506425735488648], ['MD 8', 'D', 0.9506425735488648], ['ME 1', 'D', 0.952889508576349], ['ME 2', 'R', 0.7169358695646274], ['MI 1', 'R', 0.8053565732918784], ['MI 10', 'R', 0.956558075018181], ['MI 11', 'R', 0.6378429863981835], ['MI 12', 'D', 0.9506425735488648], ['MI 13', 'D', 0.5600817848351369], ['MI 14', 'D', 1], ['MI 2', 'R', 0.8404665825755984], ['MI 3', 'R', 0.9556521950534297], ['MI 4', 'R', 0.9585054031171008], ['MI 5', 'D', 0.9506425735488648], ['MI 6', 'R', 0.7529192277273964], ['MI 7', 'R', 0.7447772962397144], ['MI 8', 'R', 0.6812704452296898], ['MI 9', 'D', 0.8460850756919668], ['MN 1', 'R', 0.7997652216143328], ['MN 2', 'R', 0.6163727417954237], ['MN 3', 'R', 0.9374520629638735], ['MN 4', 'D', 0.6627170905818172], ['MN 5', 'R', 0.5369139988888894], ['MN 6', 'R', 0.9506425735488648], ['MN 7', 'D', 0.9278340934545071], ['MN 8', 'R', 0.810320969862261], ['MO 1', 'D', 0.9506425735488648], ['MO 2', 'R', 0.7274097876983657], ['MO 3', 'R', 0.9559828316937289], ['MO 4', 'R', 0.9506425735488648], ['MO 5', 'D', 0.949553092349436], ['MO 6', 'R', 0.9506425735488648], ['MO 7', 'R', 0.9506425735488648], ['MO 8', 'R', 0.9596775167177857], ['MS 1', 'R', 0.9506425735488648], ['MS 2', 'D', 0.9596775167177857], ['MS 3', 'R', 0.8179714912502822], ['MS 4', 'R', 0.9596775167177857], ['MT 0', 'R', 0.6293051602476322], ['NC 1', 'D', 0.9596775167177857], ['NC 10', 'R', 0.9433111765614095], ['NC 11', 'R', 0.9506425735488648], ['NC 12', 'D', 0.9456371367964183], ['NC 13', 'R', 0.6799108149537879], ['NC 2', 'R', 0.8310856591205195], ['NC 3', 'R', 1], ['NC 4', 'D', 0.9596775167177857], ['NC 5', 'R', 0.9514874125013002], ['NC 6', 'R', 0.9506425735488648], ['NC 7', 'R', 0.8322934378153642], ['NC 8', 'R', 0.8822890554943235], ['NC 9', 'R', 0.6287272191165139], ['ND 0', 'R', 0.612468357040514], ['NE 1', 'R', 0.9506425735488648], ['NE 2', 'R', 0.693457997917461], ['NE 3', 'R', 0.9506425735488648], ['NH 1', 'R', 0.5051012017558189], ['NH 2', 'D', 0.7882260225515099], ['NJ 1', 'D', 0.9585054031171008], ['NJ 10', 'D', 0.9536680228900702], ['NJ 11', 'R', 0.5481305675810223], ['NJ 12', 'D', 0.9506425735488648], ['NJ 2', 'D', 0.5808145342608322], ['NJ 3', 'R', 0.7133402359561392], ['NJ 4', 'R', 0.7731228306636753], ['NJ 5', 'D', 0.8850972291282208], ['NJ 6', 'D', 0.9482788450787534], ['NJ 7', 'R', 0.6884189570993557], ['NJ 8', 'D', 0.9333636327064225], ['NJ 9', 'D', 1], ['NM 1', 'D', 0.8199979208916695], ['NM 2', 'R', 0.6184978033463949], ['NM 3', 'D', 0.8443289723579571], ['NV 1', 'D', 0.9585054031171008], ['NV 2', 'R', 0.949553092349436], ['NV 3', 'D', 0.540733547798934], ['NV 4', 'D', 0.5181982410351502], ['NY 1', 'R', 0.7162648827379289], ['NY 10', 'D', 0.9596775167177857], ['NY 11', 'R', 0.6797711373833336], ['NY 12', 'D', 0.9541727656330007], ['NY 13', 'D', 0.9596775167177857], ['NY 14', 'D', 0.6760868931888653], ['NY 15', 'D', 0.8575566354921285], ['NY 16', 'D', 1], ['NY 17', 'D', 0.9596775167177857], ['NY 18', 'D', 0.9374520629638735], ['NY 19', 'R', 0.7128814519301756], ['NY 2', 'R', 0.7448608940388892], ['NY 20', 'D', 0.9506425735488648], ['NY 21', 'R', 0.7804973732726548], ['NY 22', 'R', 0.7258425854406315], ['NY 23', 'R', 0.8239798034589163], ['NY 24', 'R', 0.7467175440829156], ['NY 25', 'D', 0.6447880797576834], ['NY 26', 'D', 0.9506425735488648], ['NY 27', 'D', 0.5443664143665239], ['NY 3', 'D', 0.9153970590082292], ['NY 4', 'D', 0.955182097525357], ['NY 5', 'D', 1], ['NY 6', 'D', 0.9596775167177857], ['NY 7', 'D', 0.5599698752398499], ['NY 8', 'D', 0.9506425735488648], ['NY 9', 'D', 0.9596775167177857], ['OH 1', 'R', 0.6901160634348864], ['OH 10', 'R', 0.7786648593021448], ['OH 11', 'D', 0.9506425735488648], ['OH 12', 'R', 0.687752334964775], ['OH 13', 'D', 0.9596775167177857], ['OH 14', 'R', 0.7465648576533299], ['OH 15', 'R', 0.783449470480432], ['OH 16', 'R', 0.8460850756919668], ['OH 2', 'R', 0.8923842306632332], ['OH 3', 'D', 0.9506425735488648], ['OH 4', 'R', 0.8617872348056004], ['OH 5', 'R', 0.9596775167177857], ['OH 6', 'R', 0.9488864702148553], ['OH 7', 'R', 0.6447111567613617], ['OH 8', 'R', 0.9506425735488648], ['OH 9', 'D', 0.9506425735488648], ['OK 1', 'R', 0.8199979208916695], ['OK 2', 'R', 0.9506425735488648], ['OK 3', 'R', 0.9506425735488648], ['OK 4', 'R', 0.9596775167177857], ['OK 5', 'R', 0.7500995773795619], ['OR 1', 'D', 0.9506425735488648], ['OR 2', 'R', 0.8255505758695553], ['OR 3', 'D', 0.6930664586690951], ['OR 4', 'D', 0.949553092349436], ['OR 5', 'D', 0.9517986667176036], ['PA 1', 'R', 0.5397191065475352], ['PA 10', 'R', 0.7747416327029665], ['PA 11', 'R', 0.7477458373042194], ['PA 12', 'R', 0.955182097525357], ['PA 13', 'R', 0.5989204822728877], ['PA 14', 'D', 0.6366960507533427], ['PA 15', 'R', 0.8327205090636502], ['PA 16', 'R', 0.756374259314707], ['PA 17', 'R', 0.5262790918353628], ['PA 18', 'D', 1], ['PA 2', 'D', 0.9506425735488648], ['PA 3', 'D', 0.7793038019046118], ['PA 4', 'R', 0.5858241885958847], ['PA 5', 'R', 0.5814556337245427], ['PA 6', 'D', 0.5989204822728877], ['PA 7', 'R', 0.8416365908434819], ['PA 8', 'D', 0.5730404726614835], ['PA 9', 'R', 0.7608018809778639], ['RI 1', 'D', 0.9506425735488648], ['RI 2', 'D', 0.9585054031171008], ['SC 1', 'R', 0.6561923557596931], ['SC 2', 'R', 0.9585054031171008], ['SC 3', 'R', 0.9506425735488648], ['SC 4', 'R', 0.5548286123469013], ['SC 5', 'R', 0.6943613731073952], ['SC 6', 'D', 0.9488864702148553], ['SC 7', 'R', 0.9506425735488648], ['SD 0', 'R', 0.5165108227422079], ['TN 1', 'R', 0.9270970029946706], ['TN 2', 'R', 0.8460850756919668], ['TN 3', 'R', 0.9456371367964183], ['TN 4', 'R', 0.7970750905801168], ['TN 5', 'D', 0.9473276667159218], ['TN 6', 'R', 0.8382567227386286], ['TN 7', 'R', 0.8108619052766545], ['TN 8', 'R', 0.9488864702148553], ['TN 9', 'D', 0.9140729979348151], ['TX 1', 'R', 0.9506425735488648], ['TX 10', 'R', 0.9477969890154264], ['TX 11', 'R', 0.9411303332090876], ['TX 12', 'R', 0.9596775167177857], ['TX 13', 'R', 0.9596775167177857], ['TX 14', 'R', 0.955182097525357], ['TX 15', 'D', 0.8366311540910354], ['TX 16', 'D', 0.8460850756919668], ['TX 17', 'R', 0.9506425735488648], ['TX 18', 'D', 0.9506425735488648], ['TX 19', 'R', 0.9506425735488648], ['TX 2', 'D', 0.7321420984525554], ['TX 20', 'D', 0.9596775167177857], ['TX 21', 'R', 0.57338976776447], ['TX 22', 'R', 0.8184771637102759], ['TX 23', 'R', 0.668018862934428], ['TX 24', 'R', 0.949553092349436], ['TX 25', 'R', 0.888116118266843], ['TX 26', 'R', 0.9488864702148553], ['TX 27', 'R', 0.9506425735488648], ['TX 28', 'D', 0.9506425735488648], ['TX 29', 'D', 0.8278492910541217], ['TX 3', 'R', 0.8199979208916695], ['TX 30', 'D', 0.8642381706085555], ['TX 31', 'R', 0.6799108149537879], ['TX 32', 'R', 0.7232398353651182], ['TX 33', 'D', 0.9596775167177857], ['TX 34', 'D', 0.9596775167177857], ['TX 35', 'D', 0.9456371367964183], ['TX 36', 'R', 0.8420083260571264], ['TX 4', 'R', 0.9596775167177857], ['TX 5', 'R', 1], ['TX 6', 'R', 0.5999408077847098], ['TX 7', 'R', 0.6812704452296898], ['TX 8', 'R', 0.9385415441633023], ['TX 9', 'D', 0.7368249474289521], ['UT 1', 'R', 0.9506425735488648], ['UT 2', 'R', 0.9585054031171008], ['UT 3', 'R', 0.9506425735488648], ['UT 4', 'R', 0.6786816561839047], ['VA 1', 'R', 0.9554556444109575], ['VA 10', 'R', 0.6751257910073715], ['VA 11', 'D', 0.8982878969250031], ['VA 2', 'R', 0.7229179386588419], ['VA 3', 'D', 1], ['VA 4', 'D', 0.952889508576349], ['VA 5', 'R', 0.5425502816388943], ['VA 6', 'R', 0.8353699611317302], ['VA 7', 'R', 0.6831256305449791], ['VA 8', 'D', 0.9488864702148553], ['VA 9', 'R', 0.7882062156925436], ['VT 0', 'D', 0.8550114325004877], ['WA 1', 'D', 0.9503483332672702], ['WA 10', 'D', 0.955182097525357], ['WA 2', 'D', 0.9506425735488648], ['WA 3', 'R', 0.7395072173235979], ['WA 4', 'R', 0.9566277580116427], ['WA 5', 'R', 0.7133402359561392], ['WA 6', 'D', 0.9514874125013002], ['WA 7', 'D', 0.9506425735488648], ['WA 8', 'R', 0.6378429863981835], ['WA 9', 'D', 0.5167763185831494], ['WI 1', 'R', 0.5847019415467071], ['WI 2', 'D', 1], ['WI 3', 'D', 0.9465227417447438], ['WI 4', 'D', 0.9506425735488648], ['WI 5', 'R', 0.9333636327064225], ['WI 6', 'R', 0.6799108149537879], ['WI 7', 'R', 0.9385415441633023], ['WI 8', 'R', 0.9482788450787534], ['WV 1', 'R', 0.9596775167177857], ['WV 2', 'R', 0.8862749252092398], ['WV 3', 'R', 0.6451695097492218], ['WY 0', 'R', 0.9506425735488648]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "masterlist = sorted(masterlist)\n",
    "\n",
    "with open('finaletreesarelife.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "    for row in masterlist:\n",
    "        spamwriter.writerow(row)\n",
    "\n",
    "print(sorted(masterlist))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
